{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "# loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, np.array(1)), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 990.349748\n",
      "Epoch 1, loss: 960.474339\n",
      "Epoch 2, loss: 1117.895552\n",
      "Epoch 3, loss: 805.534902\n",
      "Epoch 4, loss: 998.654468\n",
      "Epoch 5, loss: 984.046805\n",
      "Epoch 6, loss: 967.838460\n",
      "Epoch 7, loss: 994.983572\n",
      "Epoch 8, loss: 965.671002\n",
      "Epoch 9, loss: 940.915788\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11a8feb00>,\n",
       " <matplotlib.lines.Line2D at 0x11a8fecf8>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.106\n",
      "Epoch 0, loss: 709.417762\n",
      "Epoch 1, loss: 878.676700\n",
      "Epoch 2, loss: 1004.697085\n",
      "Epoch 3, loss: 806.923815\n",
      "Epoch 4, loss: 949.274924\n",
      "Epoch 5, loss: 882.866423\n",
      "Epoch 6, loss: 916.066056\n",
      "Epoch 7, loss: 867.505560\n",
      "Epoch 8, loss: 766.241315\n",
      "Epoch 9, loss: 899.954628\n",
      "Epoch 10, loss: 811.052468\n",
      "Epoch 11, loss: 899.645544\n",
      "Epoch 12, loss: 828.417845\n",
      "Epoch 13, loss: 858.530005\n",
      "Epoch 14, loss: 906.416982\n",
      "Epoch 15, loss: 885.264229\n",
      "Epoch 16, loss: 908.914871\n",
      "Epoch 17, loss: 942.836598\n",
      "Epoch 18, loss: 783.323236\n",
      "Epoch 19, loss: 952.931391\n",
      "Epoch 20, loss: 836.209969\n",
      "Epoch 21, loss: 1055.254539\n",
      "Epoch 22, loss: 942.074216\n",
      "Epoch 23, loss: 830.195819\n",
      "Epoch 24, loss: 900.679657\n",
      "Epoch 25, loss: 866.889627\n",
      "Epoch 26, loss: 913.177136\n",
      "Epoch 27, loss: 930.862835\n",
      "Epoch 28, loss: 776.196752\n",
      "Epoch 29, loss: 963.191424\n",
      "Epoch 30, loss: 1053.557470\n",
      "Epoch 31, loss: 970.104826\n",
      "Epoch 32, loss: 779.622029\n",
      "Epoch 33, loss: 992.935093\n",
      "Epoch 34, loss: 1149.791136\n",
      "Epoch 35, loss: 920.385895\n",
      "Epoch 36, loss: 1192.135919\n",
      "Epoch 37, loss: 911.040840\n",
      "Epoch 38, loss: 722.258121\n",
      "Epoch 39, loss: 841.740203\n",
      "Epoch 40, loss: 940.408321\n",
      "Epoch 41, loss: 818.691586\n",
      "Epoch 42, loss: 834.850846\n",
      "Epoch 43, loss: 888.524278\n",
      "Epoch 44, loss: 962.988461\n",
      "Epoch 45, loss: 776.197066\n",
      "Epoch 46, loss: 805.836058\n",
      "Epoch 47, loss: 878.388269\n",
      "Epoch 48, loss: 904.868272\n",
      "Epoch 49, loss: 945.131182\n",
      "Epoch 50, loss: 993.327996\n",
      "Epoch 51, loss: 750.363809\n",
      "Epoch 52, loss: 875.631053\n",
      "Epoch 53, loss: 881.498923\n",
      "Epoch 54, loss: 939.951104\n",
      "Epoch 55, loss: 804.682504\n",
      "Epoch 56, loss: 1033.149186\n",
      "Epoch 57, loss: 1049.416002\n",
      "Epoch 58, loss: 1026.948957\n",
      "Epoch 59, loss: 876.419283\n",
      "Epoch 60, loss: 801.470023\n",
      "Epoch 61, loss: 867.121541\n",
      "Epoch 62, loss: 962.612770\n",
      "Epoch 63, loss: 869.354988\n",
      "Epoch 64, loss: 1001.896701\n",
      "Epoch 65, loss: 916.405148\n",
      "Epoch 66, loss: 955.311622\n",
      "Epoch 67, loss: 973.393297\n",
      "Epoch 68, loss: 778.203074\n",
      "Epoch 69, loss: 963.192007\n",
      "Epoch 70, loss: 1005.310110\n",
      "Epoch 71, loss: 903.187023\n",
      "Epoch 72, loss: 835.430988\n",
      "Epoch 73, loss: 829.652689\n",
      "Epoch 74, loss: 792.873995\n",
      "Epoch 75, loss: 950.865694\n",
      "Epoch 76, loss: 1019.432543\n",
      "Epoch 77, loss: 911.027737\n",
      "Epoch 78, loss: 862.804480\n",
      "Epoch 79, loss: 743.766580\n",
      "Epoch 80, loss: 940.805568\n",
      "Epoch 81, loss: 873.771585\n",
      "Epoch 82, loss: 856.499965\n",
      "Epoch 83, loss: 781.913534\n",
      "Epoch 84, loss: 977.862409\n",
      "Epoch 85, loss: 926.126708\n",
      "Epoch 86, loss: 812.051263\n",
      "Epoch 87, loss: 875.994018\n",
      "Epoch 88, loss: 924.690515\n",
      "Epoch 89, loss: 919.543571\n",
      "Epoch 90, loss: 931.978147\n",
      "Epoch 91, loss: 934.551378\n",
      "Epoch 92, loss: 978.692318\n",
      "Epoch 93, loss: 857.546067\n",
      "Epoch 94, loss: 832.851856\n",
      "Epoch 95, loss: 935.170850\n",
      "Epoch 96, loss: 875.774186\n",
      "Epoch 97, loss: 857.506044\n",
      "Epoch 98, loss: 930.760808\n",
      "Epoch 99, loss: 839.550303\n",
      "Accuracy after training for 100 epochs:  0.156\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 971.966726\n",
      "Epoch 1, loss: 881.255680\n",
      "Epoch 2, loss: 822.309094\n",
      "Epoch 3, loss: 946.137392\n",
      "Epoch 4, loss: 1091.136520\n",
      "Epoch 5, loss: 874.283318\n",
      "Epoch 6, loss: 876.654545\n",
      "Epoch 7, loss: 992.741840\n",
      "Epoch 8, loss: 982.746273\n",
      "Epoch 9, loss: 800.137422\n",
      "Epoch 10, loss: 901.733074\n",
      "Epoch 11, loss: 838.125784\n",
      "Epoch 12, loss: 792.430506\n",
      "Epoch 13, loss: 1009.397009\n",
      "Epoch 14, loss: 885.335008\n",
      "Epoch 15, loss: 770.245175\n",
      "Epoch 16, loss: 914.646438\n",
      "Epoch 17, loss: 879.523202\n",
      "Epoch 18, loss: 1048.021301\n",
      "Epoch 19, loss: 1029.878980\n",
      "Epoch 20, loss: 861.151600\n",
      "Epoch 21, loss: 933.703469\n",
      "Epoch 22, loss: 1053.195049\n",
      "Epoch 23, loss: 888.946948\n",
      "Epoch 24, loss: 808.314295\n",
      "Epoch 25, loss: 739.817964\n",
      "Epoch 26, loss: 910.591022\n",
      "Epoch 27, loss: 824.284249\n",
      "Epoch 28, loss: 1032.727598\n",
      "Epoch 29, loss: 910.926780\n",
      "Epoch 30, loss: 884.842614\n",
      "Epoch 31, loss: 819.832188\n",
      "Epoch 32, loss: 891.657752\n",
      "Epoch 33, loss: 766.917349\n",
      "Epoch 34, loss: 1068.757265\n",
      "Epoch 35, loss: 943.204402\n",
      "Epoch 36, loss: 894.911403\n",
      "Epoch 37, loss: 1036.199323\n",
      "Epoch 38, loss: 885.198962\n",
      "Epoch 39, loss: 824.898577\n",
      "Epoch 40, loss: 840.176890\n",
      "Epoch 41, loss: 843.527188\n",
      "Epoch 42, loss: 982.313208\n",
      "Epoch 43, loss: 971.819038\n",
      "Epoch 44, loss: 1008.128820\n",
      "Epoch 45, loss: 860.012760\n",
      "Epoch 46, loss: 987.398286\n",
      "Epoch 47, loss: 933.535483\n",
      "Epoch 48, loss: 737.711952\n",
      "Epoch 49, loss: 1126.597114\n",
      "Epoch 50, loss: 840.623899\n",
      "Epoch 51, loss: 1050.646772\n",
      "Epoch 52, loss: 991.357392\n",
      "Epoch 53, loss: 861.696589\n",
      "Epoch 54, loss: 732.257154\n",
      "Epoch 55, loss: 1047.382083\n",
      "Epoch 56, loss: 1022.268852\n",
      "Epoch 57, loss: 837.198715\n",
      "Epoch 58, loss: 846.918826\n",
      "Epoch 59, loss: 764.326048\n",
      "Epoch 60, loss: 850.454190\n",
      "Epoch 61, loss: 1097.412776\n",
      "Epoch 62, loss: 815.194340\n",
      "Epoch 63, loss: 894.846031\n",
      "Epoch 64, loss: 762.687205\n",
      "Epoch 65, loss: 970.364484\n",
      "Epoch 66, loss: 873.667627\n",
      "Epoch 67, loss: 896.674599\n",
      "Epoch 68, loss: 958.427579\n",
      "Epoch 69, loss: 818.587207\n",
      "Epoch 70, loss: 913.834080\n",
      "Epoch 71, loss: 803.021543\n",
      "Epoch 72, loss: 1000.070160\n",
      "Epoch 73, loss: 1183.101808\n",
      "Epoch 74, loss: 1063.478618\n",
      "Epoch 75, loss: 943.023506\n",
      "Epoch 76, loss: 807.855486\n",
      "Epoch 77, loss: 910.484361\n",
      "Epoch 78, loss: 871.548683\n",
      "Epoch 79, loss: 797.025970\n",
      "Epoch 80, loss: 914.619864\n",
      "Epoch 81, loss: 887.805325\n",
      "Epoch 82, loss: 884.330189\n",
      "Epoch 83, loss: 845.519931\n",
      "Epoch 84, loss: 986.983079\n",
      "Epoch 85, loss: 805.825876\n",
      "Epoch 86, loss: 885.489495\n",
      "Epoch 87, loss: 1097.010104\n",
      "Epoch 88, loss: 948.022139\n",
      "Epoch 89, loss: 848.385268\n",
      "Epoch 90, loss: 935.955275\n",
      "Epoch 91, loss: 883.190041\n",
      "Epoch 92, loss: 1215.882337\n",
      "Epoch 93, loss: 757.999381\n",
      "Epoch 94, loss: 828.482680\n",
      "Epoch 95, loss: 932.625535\n",
      "Epoch 96, loss: 953.385694\n",
      "Epoch 97, loss: 979.621752\n",
      "Epoch 98, loss: 818.114554\n",
      "Epoch 99, loss: 818.666326\n",
      "Epoch 100, loss: 807.518667\n",
      "Epoch 101, loss: 1074.260534\n",
      "Epoch 102, loss: 937.104474\n",
      "Epoch 103, loss: 1141.965260\n",
      "Epoch 104, loss: 839.759985\n",
      "Epoch 105, loss: 893.779452\n",
      "Epoch 106, loss: 815.388558\n",
      "Epoch 107, loss: 909.416052\n",
      "Epoch 108, loss: 767.328591\n",
      "Epoch 109, loss: 1072.537680\n",
      "Epoch 110, loss: 849.078392\n",
      "Epoch 111, loss: 1163.209413\n",
      "Epoch 112, loss: 854.618807\n",
      "Epoch 113, loss: 951.555002\n",
      "Epoch 114, loss: 727.644487\n",
      "Epoch 115, loss: 819.493978\n",
      "Epoch 116, loss: 808.365788\n",
      "Epoch 117, loss: 777.292292\n",
      "Epoch 118, loss: 969.552555\n",
      "Epoch 119, loss: 870.712878\n",
      "Epoch 120, loss: 837.229431\n",
      "Epoch 121, loss: 896.055754\n",
      "Epoch 122, loss: 937.594941\n",
      "Epoch 123, loss: 732.575544\n",
      "Epoch 124, loss: 1047.171510\n",
      "Epoch 125, loss: 805.361423\n",
      "Epoch 126, loss: 961.044281\n",
      "Epoch 127, loss: 853.320677\n",
      "Epoch 128, loss: 832.694192\n",
      "Epoch 129, loss: 886.726068\n",
      "Epoch 130, loss: 975.601083\n",
      "Epoch 131, loss: 847.454854\n",
      "Epoch 132, loss: 1060.989281\n",
      "Epoch 133, loss: 905.212217\n",
      "Epoch 134, loss: 968.821638\n",
      "Epoch 135, loss: 957.213762\n",
      "Epoch 136, loss: 1091.652831\n",
      "Epoch 137, loss: 895.285290\n",
      "Epoch 138, loss: 849.623643\n",
      "Epoch 139, loss: 729.057064\n",
      "Epoch 140, loss: 874.231927\n",
      "Epoch 141, loss: 901.009062\n",
      "Epoch 142, loss: 1027.612231\n",
      "Epoch 143, loss: 849.672808\n",
      "Epoch 144, loss: 871.069269\n",
      "Epoch 145, loss: 830.226021\n",
      "Epoch 146, loss: 895.385874\n",
      "Epoch 147, loss: 857.789398\n",
      "Epoch 148, loss: 1027.757124\n",
      "Epoch 149, loss: 891.768434\n",
      "Epoch 150, loss: 987.632854\n",
      "Epoch 151, loss: 727.105190\n",
      "Epoch 152, loss: 858.158055\n",
      "Epoch 153, loss: 835.085766\n",
      "Epoch 154, loss: 911.898798\n",
      "Epoch 155, loss: 961.400672\n",
      "Epoch 156, loss: 994.382178\n",
      "Epoch 157, loss: 919.974126\n",
      "Epoch 158, loss: 948.909257\n",
      "Epoch 159, loss: 783.814568\n",
      "Epoch 160, loss: 891.725445\n",
      "Epoch 161, loss: 903.091257\n",
      "Epoch 162, loss: 999.891710\n",
      "Epoch 163, loss: 906.267955\n",
      "Epoch 164, loss: 764.274834\n",
      "Epoch 165, loss: 937.069281\n",
      "Epoch 166, loss: 762.675403\n",
      "Epoch 167, loss: 804.400084\n",
      "Epoch 168, loss: 823.934094\n",
      "Epoch 169, loss: 896.305429\n",
      "Epoch 170, loss: 862.192638\n",
      "Epoch 171, loss: 981.628573\n",
      "Epoch 172, loss: 806.272148\n",
      "Epoch 173, loss: 935.252349\n",
      "Epoch 174, loss: 843.152025\n",
      "Epoch 175, loss: 830.227153\n",
      "Epoch 176, loss: 829.479208\n",
      "Epoch 177, loss: 932.836563\n",
      "Epoch 178, loss: 955.318232\n",
      "Epoch 179, loss: 968.118214\n",
      "Epoch 180, loss: 919.712587\n",
      "Epoch 181, loss: 744.155799\n",
      "Epoch 182, loss: 895.262637\n",
      "Epoch 183, loss: 946.530320\n",
      "Epoch 184, loss: 959.341149\n",
      "Epoch 185, loss: 797.978099\n",
      "Epoch 186, loss: 995.509415\n",
      "Epoch 187, loss: 849.954797\n",
      "Epoch 188, loss: 983.399061\n",
      "Epoch 189, loss: 939.936836\n",
      "Epoch 190, loss: 850.709858\n",
      "Epoch 191, loss: 826.143752\n",
      "Epoch 192, loss: 837.439525\n",
      "Epoch 193, loss: 967.825518\n",
      "Epoch 194, loss: 877.283450\n",
      "Epoch 195, loss: 738.648702\n",
      "Epoch 196, loss: 745.594673\n",
      "Epoch 197, loss: 793.916550\n",
      "Epoch 198, loss: 969.163998\n",
      "Epoch 199, loss: 773.564875\n",
      "Epoch 0, loss: 956.384621\n",
      "Epoch 1, loss: 924.136971\n",
      "Epoch 2, loss: 913.180021\n",
      "Epoch 3, loss: 1178.093094\n",
      "Epoch 4, loss: 1095.502761\n",
      "Epoch 5, loss: 841.021605\n",
      "Epoch 6, loss: 886.286653\n",
      "Epoch 7, loss: 747.311082\n",
      "Epoch 8, loss: 934.655167\n",
      "Epoch 9, loss: 930.675419\n",
      "Epoch 10, loss: 916.019167\n",
      "Epoch 11, loss: 849.735678\n",
      "Epoch 12, loss: 799.576882\n",
      "Epoch 13, loss: 1046.724501\n",
      "Epoch 14, loss: 973.126486\n",
      "Epoch 15, loss: 1337.523616\n",
      "Epoch 16, loss: 881.114923\n",
      "Epoch 17, loss: 973.649450\n",
      "Epoch 18, loss: 904.922577\n",
      "Epoch 19, loss: 1078.293391\n",
      "Epoch 20, loss: 885.866681\n",
      "Epoch 21, loss: 845.716710\n",
      "Epoch 22, loss: 923.332820\n",
      "Epoch 23, loss: 812.462822\n",
      "Epoch 24, loss: 786.221719\n",
      "Epoch 25, loss: 871.462199\n",
      "Epoch 26, loss: 905.350120\n",
      "Epoch 27, loss: 927.373006\n",
      "Epoch 28, loss: 896.237745\n",
      "Epoch 29, loss: 857.199140\n",
      "Epoch 30, loss: 1001.678688\n",
      "Epoch 31, loss: 967.021678\n",
      "Epoch 32, loss: 1043.028522\n",
      "Epoch 33, loss: 913.409092\n",
      "Epoch 34, loss: 1266.538864\n",
      "Epoch 35, loss: 821.028557\n",
      "Epoch 36, loss: 857.535833\n",
      "Epoch 37, loss: 963.636431\n",
      "Epoch 38, loss: 804.507286\n",
      "Epoch 39, loss: 847.921714\n",
      "Epoch 40, loss: 882.716530\n",
      "Epoch 41, loss: 836.168577\n",
      "Epoch 42, loss: 817.919205\n",
      "Epoch 43, loss: 1096.915550\n",
      "Epoch 44, loss: 802.288767\n",
      "Epoch 45, loss: 1126.332762\n",
      "Epoch 46, loss: 761.685470\n",
      "Epoch 47, loss: 1006.254383\n",
      "Epoch 48, loss: 1021.160059\n",
      "Epoch 49, loss: 786.421994\n",
      "Epoch 50, loss: 819.300033\n",
      "Epoch 51, loss: 818.546830\n",
      "Epoch 52, loss: 1039.525866\n",
      "Epoch 53, loss: 826.235701\n",
      "Epoch 54, loss: 811.001052\n",
      "Epoch 55, loss: 831.633434\n",
      "Epoch 56, loss: 914.865300\n",
      "Epoch 57, loss: 885.188855\n",
      "Epoch 58, loss: 890.989630\n",
      "Epoch 59, loss: 1094.460506\n",
      "Epoch 60, loss: 902.659436\n",
      "Epoch 61, loss: 824.459134\n",
      "Epoch 62, loss: 1036.147271\n",
      "Epoch 63, loss: 885.211413\n",
      "Epoch 64, loss: 897.295462\n",
      "Epoch 65, loss: 797.643368\n",
      "Epoch 66, loss: 807.730744\n",
      "Epoch 67, loss: 791.708859\n",
      "Epoch 68, loss: 820.528971\n",
      "Epoch 69, loss: 1071.957412\n",
      "Epoch 70, loss: 925.970652\n",
      "Epoch 71, loss: 748.080215\n",
      "Epoch 72, loss: 958.325874\n",
      "Epoch 73, loss: 804.580674\n",
      "Epoch 74, loss: 885.267985\n",
      "Epoch 75, loss: 851.778273\n",
      "Epoch 76, loss: 932.574480\n",
      "Epoch 77, loss: 834.199301\n",
      "Epoch 78, loss: 936.357816\n",
      "Epoch 79, loss: 964.284439\n",
      "Epoch 80, loss: 943.025424\n",
      "Epoch 81, loss: 858.484672\n",
      "Epoch 82, loss: 910.965132\n",
      "Epoch 83, loss: 925.951640\n",
      "Epoch 84, loss: 893.332199\n",
      "Epoch 85, loss: 896.513415\n",
      "Epoch 86, loss: 1067.800366\n",
      "Epoch 87, loss: 796.366490\n",
      "Epoch 88, loss: 847.951581\n",
      "Epoch 89, loss: 719.628484\n",
      "Epoch 90, loss: 983.380667\n",
      "Epoch 91, loss: 803.862440\n",
      "Epoch 92, loss: 975.497851\n",
      "Epoch 93, loss: 1002.132234\n",
      "Epoch 94, loss: 932.373831\n",
      "Epoch 95, loss: 905.278504\n",
      "Epoch 96, loss: 850.674624\n",
      "Epoch 97, loss: 1080.518798\n",
      "Epoch 98, loss: 830.555269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99, loss: 850.616116\n",
      "Epoch 100, loss: 870.668828\n",
      "Epoch 101, loss: 768.046673\n",
      "Epoch 102, loss: 860.168071\n",
      "Epoch 103, loss: 876.655479\n",
      "Epoch 104, loss: 768.643849\n",
      "Epoch 105, loss: 858.650152\n",
      "Epoch 106, loss: 908.248201\n",
      "Epoch 107, loss: 997.588054\n",
      "Epoch 108, loss: 805.176095\n",
      "Epoch 109, loss: 836.560770\n",
      "Epoch 110, loss: 1005.908933\n",
      "Epoch 111, loss: 979.182544\n",
      "Epoch 112, loss: 953.674011\n",
      "Epoch 113, loss: 839.023752\n",
      "Epoch 114, loss: 921.955608\n",
      "Epoch 115, loss: 814.670450\n",
      "Epoch 116, loss: 962.633922\n",
      "Epoch 117, loss: 1015.559953\n",
      "Epoch 118, loss: 981.513567\n",
      "Epoch 119, loss: 883.449967\n",
      "Epoch 120, loss: 761.715468\n",
      "Epoch 121, loss: 756.511594\n",
      "Epoch 122, loss: 825.670032\n",
      "Epoch 123, loss: 858.475683\n",
      "Epoch 124, loss: 854.976692\n",
      "Epoch 125, loss: 1035.315256\n",
      "Epoch 126, loss: 858.425668\n",
      "Epoch 127, loss: 942.742725\n",
      "Epoch 128, loss: 1058.525387\n",
      "Epoch 129, loss: 862.755773\n",
      "Epoch 130, loss: 872.307785\n",
      "Epoch 131, loss: 724.503939\n",
      "Epoch 132, loss: 960.969453\n",
      "Epoch 133, loss: 874.174841\n",
      "Epoch 134, loss: 811.583237\n",
      "Epoch 135, loss: 1075.088459\n",
      "Epoch 136, loss: 794.023867\n",
      "Epoch 137, loss: 795.173074\n",
      "Epoch 138, loss: 889.621530\n",
      "Epoch 139, loss: 902.490811\n",
      "Epoch 140, loss: 797.236736\n",
      "Epoch 141, loss: 866.218013\n",
      "Epoch 142, loss: 1029.756141\n",
      "Epoch 143, loss: 942.138290\n",
      "Epoch 144, loss: 878.350052\n",
      "Epoch 145, loss: 910.223895\n",
      "Epoch 146, loss: 910.533261\n",
      "Epoch 147, loss: 1029.761974\n",
      "Epoch 148, loss: 828.637438\n",
      "Epoch 149, loss: 1185.043228\n",
      "Epoch 150, loss: 812.006719\n",
      "Epoch 151, loss: 960.860599\n",
      "Epoch 152, loss: 897.308076\n",
      "Epoch 153, loss: 822.722835\n",
      "Epoch 154, loss: 784.873809\n",
      "Epoch 155, loss: 951.363154\n",
      "Epoch 156, loss: 858.348819\n",
      "Epoch 157, loss: 815.258883\n",
      "Epoch 158, loss: 801.599402\n",
      "Epoch 159, loss: 928.165834\n",
      "Epoch 160, loss: 869.875583\n",
      "Epoch 161, loss: 820.481735\n",
      "Epoch 162, loss: 930.468382\n",
      "Epoch 163, loss: 879.079214\n",
      "Epoch 164, loss: 824.682809\n",
      "Epoch 165, loss: 813.859215\n",
      "Epoch 166, loss: 919.308155\n",
      "Epoch 167, loss: 853.510604\n",
      "Epoch 168, loss: 879.470707\n",
      "Epoch 169, loss: 916.749393\n",
      "Epoch 170, loss: 954.831145\n",
      "Epoch 171, loss: 804.186822\n",
      "Epoch 172, loss: 928.824476\n",
      "Epoch 173, loss: 890.332705\n",
      "Epoch 174, loss: 947.554999\n",
      "Epoch 175, loss: 830.930382\n",
      "Epoch 176, loss: 949.888101\n",
      "Epoch 177, loss: 823.243038\n",
      "Epoch 178, loss: 875.385133\n",
      "Epoch 179, loss: 817.539270\n",
      "Epoch 180, loss: 965.660384\n",
      "Epoch 181, loss: 1217.714711\n",
      "Epoch 182, loss: 842.735676\n",
      "Epoch 183, loss: 765.522328\n",
      "Epoch 184, loss: 953.271165\n",
      "Epoch 185, loss: 895.857300\n",
      "Epoch 186, loss: 873.040264\n",
      "Epoch 187, loss: 965.472139\n",
      "Epoch 188, loss: 735.801938\n",
      "Epoch 189, loss: 778.513956\n",
      "Epoch 190, loss: 799.387334\n",
      "Epoch 191, loss: 1019.408145\n",
      "Epoch 192, loss: 1090.776418\n",
      "Epoch 193, loss: 1008.448881\n",
      "Epoch 194, loss: 737.509755\n",
      "Epoch 195, loss: 1030.974475\n",
      "Epoch 196, loss: 932.551125\n",
      "Epoch 197, loss: 1016.504579\n",
      "Epoch 198, loss: 816.206495\n",
      "Epoch 199, loss: 992.035601\n",
      "Epoch 0, loss: 822.439463\n",
      "Epoch 1, loss: 878.890957\n",
      "Epoch 2, loss: 874.155429\n",
      "Epoch 3, loss: 915.056945\n",
      "Epoch 4, loss: 1008.048862\n",
      "Epoch 5, loss: 988.005206\n",
      "Epoch 6, loss: 1045.689335\n",
      "Epoch 7, loss: 743.969002\n",
      "Epoch 8, loss: 843.035366\n",
      "Epoch 9, loss: 875.652297\n",
      "Epoch 10, loss: 953.372029\n",
      "Epoch 11, loss: 970.620651\n",
      "Epoch 12, loss: 820.003474\n",
      "Epoch 13, loss: 867.947096\n",
      "Epoch 14, loss: 904.162587\n",
      "Epoch 15, loss: 892.760281\n",
      "Epoch 16, loss: 757.827462\n",
      "Epoch 17, loss: 813.011876\n",
      "Epoch 18, loss: 904.428751\n",
      "Epoch 19, loss: 1194.046034\n",
      "Epoch 20, loss: 979.754642\n",
      "Epoch 21, loss: 990.261423\n",
      "Epoch 22, loss: 1057.262684\n",
      "Epoch 23, loss: 725.744242\n",
      "Epoch 24, loss: 883.464982\n",
      "Epoch 25, loss: 993.842365\n",
      "Epoch 26, loss: 874.112821\n",
      "Epoch 27, loss: 797.799506\n",
      "Epoch 28, loss: 892.601451\n",
      "Epoch 29, loss: 768.054747\n",
      "Epoch 30, loss: 914.262003\n",
      "Epoch 31, loss: 911.718972\n",
      "Epoch 32, loss: 845.656145\n",
      "Epoch 33, loss: 748.833363\n",
      "Epoch 34, loss: 1121.956057\n",
      "Epoch 35, loss: 874.927858\n",
      "Epoch 36, loss: 828.411036\n",
      "Epoch 37, loss: 842.744676\n",
      "Epoch 38, loss: 812.573190\n",
      "Epoch 39, loss: 745.959261\n",
      "Epoch 40, loss: 895.971772\n",
      "Epoch 41, loss: 778.857035\n",
      "Epoch 42, loss: 930.672100\n",
      "Epoch 43, loss: 921.975860\n",
      "Epoch 44, loss: 975.142371\n",
      "Epoch 45, loss: 1041.157809\n",
      "Epoch 46, loss: 775.141533\n",
      "Epoch 47, loss: 1019.877578\n",
      "Epoch 48, loss: 895.493243\n",
      "Epoch 49, loss: 926.532883\n",
      "Epoch 50, loss: 781.853130\n",
      "Epoch 51, loss: 784.157969\n",
      "Epoch 52, loss: 974.676314\n",
      "Epoch 53, loss: 924.625648\n",
      "Epoch 54, loss: 835.327821\n",
      "Epoch 55, loss: 1062.383381\n",
      "Epoch 56, loss: 933.428847\n",
      "Epoch 57, loss: 969.837734\n",
      "Epoch 58, loss: 902.647599\n",
      "Epoch 59, loss: 733.890984\n",
      "Epoch 60, loss: 994.510424\n",
      "Epoch 61, loss: 940.744720\n",
      "Epoch 62, loss: 890.013342\n",
      "Epoch 63, loss: 1045.144192\n",
      "Epoch 64, loss: 822.242366\n",
      "Epoch 65, loss: 975.453125\n",
      "Epoch 66, loss: 846.107392\n",
      "Epoch 67, loss: 897.632497\n",
      "Epoch 68, loss: 782.124560\n",
      "Epoch 69, loss: 908.118847\n",
      "Epoch 70, loss: 891.297268\n",
      "Epoch 71, loss: 774.393309\n",
      "Epoch 72, loss: 990.371993\n",
      "Epoch 73, loss: 785.985180\n",
      "Epoch 74, loss: 944.927014\n",
      "Epoch 75, loss: 1046.439540\n",
      "Epoch 76, loss: 1006.527793\n",
      "Epoch 77, loss: 897.972528\n",
      "Epoch 78, loss: 1018.086635\n",
      "Epoch 79, loss: 799.470907\n",
      "Epoch 80, loss: 784.826826\n",
      "Epoch 81, loss: 1154.029731\n",
      "Epoch 82, loss: 782.047686\n",
      "Epoch 83, loss: 869.536435\n",
      "Epoch 84, loss: 951.714984\n",
      "Epoch 85, loss: 1016.820623\n",
      "Epoch 86, loss: 822.349326\n",
      "Epoch 87, loss: 853.973840\n",
      "Epoch 88, loss: 951.431600\n",
      "Epoch 89, loss: 911.042906\n",
      "Epoch 90, loss: 971.031066\n",
      "Epoch 91, loss: 961.044792\n",
      "Epoch 92, loss: 999.202353\n",
      "Epoch 93, loss: 958.946311\n",
      "Epoch 94, loss: 866.085641\n",
      "Epoch 95, loss: 850.779509\n",
      "Epoch 96, loss: 1222.396206\n",
      "Epoch 97, loss: 778.675096\n",
      "Epoch 98, loss: 858.571168\n",
      "Epoch 99, loss: 992.427433\n",
      "Epoch 100, loss: 970.711425\n",
      "Epoch 101, loss: 934.768908\n",
      "Epoch 102, loss: 793.068303\n",
      "Epoch 103, loss: 1053.078432\n",
      "Epoch 104, loss: 804.995335\n",
      "Epoch 105, loss: 850.386884\n",
      "Epoch 106, loss: 830.828989\n",
      "Epoch 107, loss: 953.582386\n",
      "Epoch 108, loss: 852.780983\n",
      "Epoch 109, loss: 1073.344861\n",
      "Epoch 110, loss: 911.577201\n",
      "Epoch 111, loss: 926.656708\n",
      "Epoch 112, loss: 754.977966\n",
      "Epoch 113, loss: 1063.158846\n",
      "Epoch 114, loss: 903.659505\n",
      "Epoch 115, loss: 935.451824\n",
      "Epoch 116, loss: 836.503284\n",
      "Epoch 117, loss: 849.135223\n",
      "Epoch 118, loss: 910.706056\n",
      "Epoch 119, loss: 889.212150\n",
      "Epoch 120, loss: 908.392976\n",
      "Epoch 121, loss: 746.215049\n",
      "Epoch 122, loss: 989.697416\n",
      "Epoch 123, loss: 781.022526\n",
      "Epoch 124, loss: 831.348256\n",
      "Epoch 125, loss: 869.033443\n",
      "Epoch 126, loss: 781.622765\n",
      "Epoch 127, loss: 1065.938201\n",
      "Epoch 128, loss: 1184.924024\n",
      "Epoch 129, loss: 883.982447\n",
      "Epoch 130, loss: 930.997566\n",
      "Epoch 131, loss: 1028.988170\n",
      "Epoch 132, loss: 769.551064\n",
      "Epoch 133, loss: 1075.637880\n",
      "Epoch 134, loss: 910.760937\n",
      "Epoch 135, loss: 865.196625\n",
      "Epoch 136, loss: 868.775475\n",
      "Epoch 137, loss: 933.759008\n",
      "Epoch 138, loss: 825.214097\n",
      "Epoch 139, loss: 836.560995\n",
      "Epoch 140, loss: 909.796576\n",
      "Epoch 141, loss: 777.345557\n",
      "Epoch 142, loss: 811.635856\n",
      "Epoch 143, loss: 901.496123\n",
      "Epoch 144, loss: 1074.209306\n",
      "Epoch 145, loss: 1003.334000\n",
      "Epoch 146, loss: 924.648002\n",
      "Epoch 147, loss: 896.498874\n",
      "Epoch 148, loss: 1195.735972\n",
      "Epoch 149, loss: 1049.226061\n",
      "Epoch 150, loss: 1086.519097\n",
      "Epoch 151, loss: 935.833177\n",
      "Epoch 152, loss: 834.132669\n",
      "Epoch 153, loss: 862.995835\n",
      "Epoch 154, loss: 737.161578\n",
      "Epoch 155, loss: 983.424845\n",
      "Epoch 156, loss: 970.554228\n",
      "Epoch 157, loss: 783.576642\n",
      "Epoch 158, loss: 919.581657\n",
      "Epoch 159, loss: 882.320249\n",
      "Epoch 160, loss: 902.155730\n",
      "Epoch 161, loss: 845.795328\n",
      "Epoch 162, loss: 913.087099\n",
      "Epoch 163, loss: 979.078795\n",
      "Epoch 164, loss: 923.040858\n",
      "Epoch 165, loss: 989.741445\n",
      "Epoch 166, loss: 737.529606\n",
      "Epoch 167, loss: 874.095019\n",
      "Epoch 168, loss: 792.690685\n",
      "Epoch 169, loss: 1005.991233\n",
      "Epoch 170, loss: 773.521397\n",
      "Epoch 171, loss: 831.790332\n",
      "Epoch 172, loss: 929.356279\n",
      "Epoch 173, loss: 886.805734\n",
      "Epoch 174, loss: 903.556622\n",
      "Epoch 175, loss: 989.449422\n",
      "Epoch 176, loss: 833.526874\n",
      "Epoch 177, loss: 1011.651989\n",
      "Epoch 178, loss: 879.904221\n",
      "Epoch 179, loss: 750.358106\n",
      "Epoch 180, loss: 952.159314\n",
      "Epoch 181, loss: 1047.411336\n",
      "Epoch 182, loss: 1083.016394\n",
      "Epoch 183, loss: 862.736901\n",
      "Epoch 184, loss: 910.127770\n",
      "Epoch 185, loss: 1057.116245\n",
      "Epoch 186, loss: 941.323368\n",
      "Epoch 187, loss: 859.122379\n",
      "Epoch 188, loss: 968.239786\n",
      "Epoch 189, loss: 966.186575\n",
      "Epoch 190, loss: 882.955757\n",
      "Epoch 191, loss: 917.928963\n",
      "Epoch 192, loss: 1079.826696\n",
      "Epoch 193, loss: 985.161547\n",
      "Epoch 194, loss: 879.389245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195, loss: 1030.658116\n",
      "Epoch 196, loss: 839.001950\n",
      "Epoch 197, loss: 906.806280\n",
      "Epoch 198, loss: 892.044601\n",
      "Epoch 199, loss: 858.724470\n",
      "Epoch 0, loss: 1046.565095\n",
      "Epoch 1, loss: 920.413155\n",
      "Epoch 2, loss: 783.353562\n",
      "Epoch 3, loss: 955.481161\n",
      "Epoch 4, loss: 856.046264\n",
      "Epoch 5, loss: 985.470827\n",
      "Epoch 6, loss: 784.512456\n",
      "Epoch 7, loss: 1081.306306\n",
      "Epoch 8, loss: 956.045397\n",
      "Epoch 9, loss: 810.204367\n",
      "Epoch 10, loss: 751.365222\n",
      "Epoch 11, loss: 892.621310\n",
      "Epoch 12, loss: 959.119022\n",
      "Epoch 13, loss: 1077.567037\n",
      "Epoch 14, loss: 808.156909\n",
      "Epoch 15, loss: 728.647742\n",
      "Epoch 16, loss: 765.846498\n",
      "Epoch 17, loss: 969.748976\n",
      "Epoch 18, loss: 1075.386027\n",
      "Epoch 19, loss: 920.463079\n",
      "Epoch 20, loss: 828.026406\n",
      "Epoch 21, loss: 830.099091\n",
      "Epoch 22, loss: 797.933955\n",
      "Epoch 23, loss: 865.412851\n",
      "Epoch 24, loss: 902.464253\n",
      "Epoch 25, loss: 924.158861\n",
      "Epoch 26, loss: 795.102467\n",
      "Epoch 27, loss: 845.190855\n",
      "Epoch 28, loss: 896.466257\n",
      "Epoch 29, loss: 1092.354232\n",
      "Epoch 30, loss: 990.025093\n",
      "Epoch 31, loss: 1090.690982\n",
      "Epoch 32, loss: 849.679164\n",
      "Epoch 33, loss: 976.335948\n",
      "Epoch 34, loss: 1066.221187\n",
      "Epoch 35, loss: 979.122514\n",
      "Epoch 36, loss: 849.719409\n",
      "Epoch 37, loss: 748.373072\n",
      "Epoch 38, loss: 731.600698\n",
      "Epoch 39, loss: 1100.430868\n",
      "Epoch 40, loss: 935.394636\n",
      "Epoch 41, loss: 926.287463\n",
      "Epoch 42, loss: 826.177500\n",
      "Epoch 43, loss: 902.823178\n",
      "Epoch 44, loss: 1117.900657\n",
      "Epoch 45, loss: 1057.711974\n",
      "Epoch 46, loss: 1039.034705\n",
      "Epoch 47, loss: 835.086875\n",
      "Epoch 48, loss: 853.673057\n",
      "Epoch 49, loss: 1066.994796\n",
      "Epoch 50, loss: 857.400477\n",
      "Epoch 51, loss: 804.630242\n",
      "Epoch 52, loss: 838.760221\n",
      "Epoch 53, loss: 890.166981\n",
      "Epoch 54, loss: 854.238523\n",
      "Epoch 55, loss: 953.877069\n",
      "Epoch 56, loss: 1075.893024\n",
      "Epoch 57, loss: 886.715608\n",
      "Epoch 58, loss: 984.005849\n",
      "Epoch 59, loss: 1051.567094\n",
      "Epoch 60, loss: 875.141641\n",
      "Epoch 61, loss: 884.322004\n",
      "Epoch 62, loss: 867.285358\n",
      "Epoch 63, loss: 776.194950\n",
      "Epoch 64, loss: 899.045895\n",
      "Epoch 65, loss: 862.934091\n",
      "Epoch 66, loss: 901.041875\n",
      "Epoch 67, loss: 942.141511\n",
      "Epoch 68, loss: 759.907327\n",
      "Epoch 69, loss: 1028.083578\n",
      "Epoch 70, loss: 843.412679\n",
      "Epoch 71, loss: 997.103273\n",
      "Epoch 72, loss: 853.990597\n",
      "Epoch 73, loss: 885.130635\n",
      "Epoch 74, loss: 937.866354\n",
      "Epoch 75, loss: 1206.936960\n",
      "Epoch 76, loss: 1206.572799\n",
      "Epoch 77, loss: 1142.645954\n",
      "Epoch 78, loss: 814.075201\n",
      "Epoch 79, loss: 809.340565\n",
      "Epoch 80, loss: 853.825856\n",
      "Epoch 81, loss: 942.818669\n",
      "Epoch 82, loss: 837.216707\n",
      "Epoch 83, loss: 896.217230\n",
      "Epoch 84, loss: 861.657003\n",
      "Epoch 85, loss: 901.362110\n",
      "Epoch 86, loss: 1021.341856\n",
      "Epoch 87, loss: 872.492670\n",
      "Epoch 88, loss: 932.392624\n",
      "Epoch 89, loss: 920.578633\n",
      "Epoch 90, loss: 1093.399020\n",
      "Epoch 91, loss: 957.792270\n",
      "Epoch 92, loss: 989.346938\n",
      "Epoch 93, loss: 782.226437\n",
      "Epoch 94, loss: 747.479240\n",
      "Epoch 95, loss: 1096.620711\n",
      "Epoch 96, loss: 895.621112\n",
      "Epoch 97, loss: 721.501543\n",
      "Epoch 98, loss: 824.720533\n",
      "Epoch 99, loss: 950.835370\n",
      "Epoch 100, loss: 1081.706393\n",
      "Epoch 101, loss: 935.414980\n",
      "Epoch 102, loss: 817.758427\n",
      "Epoch 103, loss: 882.187753\n",
      "Epoch 104, loss: 917.918013\n",
      "Epoch 105, loss: 962.430987\n",
      "Epoch 106, loss: 752.994587\n",
      "Epoch 107, loss: 912.167062\n",
      "Epoch 108, loss: 1018.861162\n",
      "Epoch 109, loss: 847.976166\n",
      "Epoch 110, loss: 836.020990\n",
      "Epoch 111, loss: 793.890894\n",
      "Epoch 112, loss: 954.494421\n",
      "Epoch 113, loss: 834.813665\n",
      "Epoch 114, loss: 923.639133\n",
      "Epoch 115, loss: 1044.321710\n",
      "Epoch 116, loss: 859.886902\n",
      "Epoch 117, loss: 942.616502\n",
      "Epoch 118, loss: 1031.887603\n",
      "Epoch 119, loss: 892.813639\n",
      "Epoch 120, loss: 860.241678\n",
      "Epoch 121, loss: 1046.870507\n",
      "Epoch 122, loss: 780.513628\n",
      "Epoch 123, loss: 1028.083200\n",
      "Epoch 124, loss: 881.811877\n",
      "Epoch 125, loss: 785.996002\n",
      "Epoch 126, loss: 810.400491\n",
      "Epoch 127, loss: 920.744344\n",
      "Epoch 128, loss: 1181.004079\n",
      "Epoch 129, loss: 762.290014\n",
      "Epoch 130, loss: 1005.416303\n",
      "Epoch 131, loss: 851.125393\n",
      "Epoch 132, loss: 827.019820\n",
      "Epoch 133, loss: 908.419759\n",
      "Epoch 134, loss: 1026.098871\n",
      "Epoch 135, loss: 813.468062\n",
      "Epoch 136, loss: 892.038810\n",
      "Epoch 137, loss: 873.962480\n",
      "Epoch 138, loss: 965.661409\n",
      "Epoch 139, loss: 813.917152\n",
      "Epoch 140, loss: 1000.938922\n",
      "Epoch 141, loss: 822.478421\n",
      "Epoch 142, loss: 831.230745\n",
      "Epoch 143, loss: 861.173458\n",
      "Epoch 144, loss: 859.477945\n",
      "Epoch 145, loss: 914.428680\n",
      "Epoch 146, loss: 900.857398\n",
      "Epoch 147, loss: 1070.774257\n",
      "Epoch 148, loss: 766.749520\n",
      "Epoch 149, loss: 857.271463\n",
      "Epoch 150, loss: 860.368187\n",
      "Epoch 151, loss: 922.344637\n",
      "Epoch 152, loss: 888.326785\n",
      "Epoch 153, loss: 1075.995999\n",
      "Epoch 154, loss: 878.051021\n",
      "Epoch 155, loss: 930.019213\n",
      "Epoch 156, loss: 933.448568\n",
      "Epoch 157, loss: 792.377929\n",
      "Epoch 158, loss: 923.548730\n",
      "Epoch 159, loss: 1055.629544\n",
      "Epoch 160, loss: 1068.556787\n",
      "Epoch 161, loss: 983.497882\n",
      "Epoch 162, loss: 1008.998375\n",
      "Epoch 163, loss: 901.497198\n",
      "Epoch 164, loss: 838.610007\n",
      "Epoch 165, loss: 918.780319\n",
      "Epoch 166, loss: 827.478712\n",
      "Epoch 167, loss: 981.621205\n",
      "Epoch 168, loss: 958.433424\n",
      "Epoch 169, loss: 921.662259\n",
      "Epoch 170, loss: 1021.667992\n",
      "Epoch 171, loss: 849.353164\n",
      "Epoch 172, loss: 947.016975\n",
      "Epoch 173, loss: 942.561771\n",
      "Epoch 174, loss: 806.730843\n",
      "Epoch 175, loss: 1074.521846\n",
      "Epoch 176, loss: 1043.832778\n",
      "Epoch 177, loss: 872.641269\n",
      "Epoch 178, loss: 946.779230\n",
      "Epoch 179, loss: 821.777455\n",
      "Epoch 180, loss: 950.716102\n",
      "Epoch 181, loss: 1177.742059\n",
      "Epoch 182, loss: 837.036588\n",
      "Epoch 183, loss: 896.980655\n",
      "Epoch 184, loss: 1105.170723\n",
      "Epoch 185, loss: 849.984412\n",
      "Epoch 186, loss: 839.147783\n",
      "Epoch 187, loss: 954.995995\n",
      "Epoch 188, loss: 799.929955\n",
      "Epoch 189, loss: 832.957193\n",
      "Epoch 190, loss: 753.419107\n",
      "Epoch 191, loss: 959.082936\n",
      "Epoch 192, loss: 996.280921\n",
      "Epoch 193, loss: 898.026706\n",
      "Epoch 194, loss: 927.184184\n",
      "Epoch 195, loss: 893.874581\n",
      "Epoch 196, loss: 792.467945\n",
      "Epoch 197, loss: 832.480057\n",
      "Epoch 198, loss: 772.354137\n",
      "Epoch 199, loss: 949.627661\n",
      "Epoch 0, loss: 685.995313\n",
      "Epoch 1, loss: 686.454345\n",
      "Epoch 2, loss: 684.509121\n",
      "Epoch 3, loss: 681.781995\n",
      "Epoch 4, loss: 682.830180\n",
      "Epoch 5, loss: 677.985937\n",
      "Epoch 6, loss: 678.875788\n",
      "Epoch 7, loss: 677.577615\n",
      "Epoch 8, loss: 676.731296\n",
      "Epoch 9, loss: 674.660574\n",
      "Epoch 10, loss: 675.940936\n",
      "Epoch 11, loss: 675.218028\n",
      "Epoch 12, loss: 677.726549\n",
      "Epoch 13, loss: 673.866036\n",
      "Epoch 14, loss: 676.022858\n",
      "Epoch 15, loss: 674.951329\n",
      "Epoch 16, loss: 674.378540\n",
      "Epoch 17, loss: 673.723873\n",
      "Epoch 18, loss: 675.409587\n",
      "Epoch 19, loss: 674.821433\n",
      "Epoch 20, loss: 675.289162\n",
      "Epoch 21, loss: 672.584617\n",
      "Epoch 22, loss: 673.884793\n",
      "Epoch 23, loss: 676.052116\n",
      "Epoch 24, loss: 673.493903\n",
      "Epoch 25, loss: 673.040633\n",
      "Epoch 26, loss: 673.237876\n",
      "Epoch 27, loss: 674.404384\n",
      "Epoch 28, loss: 672.387794\n",
      "Epoch 29, loss: 679.558236\n",
      "Epoch 30, loss: 673.213308\n",
      "Epoch 31, loss: 673.597917\n",
      "Epoch 32, loss: 677.308908\n",
      "Epoch 33, loss: 672.893006\n",
      "Epoch 34, loss: 673.508024\n",
      "Epoch 35, loss: 673.437824\n",
      "Epoch 36, loss: 673.113712\n",
      "Epoch 37, loss: 673.024343\n",
      "Epoch 38, loss: 675.749355\n",
      "Epoch 39, loss: 674.843884\n",
      "Epoch 40, loss: 674.078165\n",
      "Epoch 41, loss: 673.742409\n",
      "Epoch 42, loss: 673.369586\n",
      "Epoch 43, loss: 674.793595\n",
      "Epoch 44, loss: 675.425929\n",
      "Epoch 45, loss: 672.363482\n",
      "Epoch 46, loss: 674.244903\n",
      "Epoch 47, loss: 676.589515\n",
      "Epoch 48, loss: 672.267892\n",
      "Epoch 49, loss: 674.087627\n",
      "Epoch 50, loss: 674.278699\n",
      "Epoch 51, loss: 674.592961\n",
      "Epoch 52, loss: 672.904038\n",
      "Epoch 53, loss: 672.910590\n",
      "Epoch 54, loss: 674.324740\n",
      "Epoch 55, loss: 671.949200\n",
      "Epoch 56, loss: 674.439776\n",
      "Epoch 57, loss: 671.952867\n",
      "Epoch 58, loss: 671.839914\n",
      "Epoch 59, loss: 676.258030\n",
      "Epoch 60, loss: 674.860613\n",
      "Epoch 61, loss: 671.579006\n",
      "Epoch 62, loss: 675.603778\n",
      "Epoch 63, loss: 674.072969\n",
      "Epoch 64, loss: 674.251671\n",
      "Epoch 65, loss: 674.620790\n",
      "Epoch 66, loss: 673.560834\n",
      "Epoch 67, loss: 673.878472\n",
      "Epoch 68, loss: 672.347928\n",
      "Epoch 69, loss: 675.280761\n",
      "Epoch 70, loss: 673.858833\n",
      "Epoch 71, loss: 673.579906\n",
      "Epoch 72, loss: 674.607860\n",
      "Epoch 73, loss: 673.724872\n",
      "Epoch 74, loss: 673.091730\n",
      "Epoch 75, loss: 672.509417\n",
      "Epoch 76, loss: 673.450758\n",
      "Epoch 77, loss: 672.065472\n",
      "Epoch 78, loss: 673.075868\n",
      "Epoch 79, loss: 673.698574\n",
      "Epoch 80, loss: 671.450159\n",
      "Epoch 81, loss: 673.851903\n",
      "Epoch 82, loss: 672.125045\n",
      "Epoch 83, loss: 672.861101\n",
      "Epoch 84, loss: 674.598059\n",
      "Epoch 85, loss: 675.847565\n",
      "Epoch 86, loss: 674.036556\n",
      "Epoch 87, loss: 671.809779\n",
      "Epoch 88, loss: 677.275451\n",
      "Epoch 89, loss: 672.885636\n",
      "Epoch 90, loss: 673.048169\n",
      "Epoch 91, loss: 675.002768\n",
      "Epoch 92, loss: 673.547166\n",
      "Epoch 93, loss: 674.466742\n",
      "Epoch 94, loss: 673.373000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95, loss: 672.741905\n",
      "Epoch 96, loss: 674.031431\n",
      "Epoch 97, loss: 674.690989\n",
      "Epoch 98, loss: 672.897216\n",
      "Epoch 99, loss: 672.673522\n",
      "Epoch 100, loss: 675.247094\n",
      "Epoch 101, loss: 674.852668\n",
      "Epoch 102, loss: 674.299692\n",
      "Epoch 103, loss: 672.826055\n",
      "Epoch 104, loss: 672.610411\n",
      "Epoch 105, loss: 675.184021\n",
      "Epoch 106, loss: 672.736835\n",
      "Epoch 107, loss: 673.683614\n",
      "Epoch 108, loss: 673.856104\n",
      "Epoch 109, loss: 672.758352\n",
      "Epoch 110, loss: 671.649074\n",
      "Epoch 111, loss: 673.898908\n",
      "Epoch 112, loss: 675.475448\n",
      "Epoch 113, loss: 671.940967\n",
      "Epoch 114, loss: 674.081047\n",
      "Epoch 115, loss: 673.283446\n",
      "Epoch 116, loss: 677.836453\n",
      "Epoch 117, loss: 672.785229\n",
      "Epoch 118, loss: 672.255367\n",
      "Epoch 119, loss: 672.451088\n",
      "Epoch 120, loss: 672.699441\n",
      "Epoch 121, loss: 672.703701\n",
      "Epoch 122, loss: 673.737771\n",
      "Epoch 123, loss: 673.873616\n",
      "Epoch 124, loss: 672.332440\n",
      "Epoch 125, loss: 673.217938\n",
      "Epoch 126, loss: 672.179267\n",
      "Epoch 127, loss: 674.242139\n",
      "Epoch 128, loss: 672.214266\n",
      "Epoch 129, loss: 673.879029\n",
      "Epoch 130, loss: 674.743658\n",
      "Epoch 131, loss: 672.133843\n",
      "Epoch 132, loss: 672.922328\n",
      "Epoch 133, loss: 671.440262\n",
      "Epoch 134, loss: 674.609660\n",
      "Epoch 135, loss: 673.553595\n",
      "Epoch 136, loss: 673.218618\n",
      "Epoch 137, loss: 672.562191\n",
      "Epoch 138, loss: 675.160667\n",
      "Epoch 139, loss: 671.532170\n",
      "Epoch 140, loss: 673.205861\n",
      "Epoch 141, loss: 674.326551\n",
      "Epoch 142, loss: 672.070204\n",
      "Epoch 143, loss: 671.144159\n",
      "Epoch 144, loss: 671.532277\n",
      "Epoch 145, loss: 672.510429\n",
      "Epoch 146, loss: 672.027178\n",
      "Epoch 147, loss: 673.856628\n",
      "Epoch 148, loss: 675.816492\n",
      "Epoch 149, loss: 672.886352\n",
      "Epoch 150, loss: 672.274629\n",
      "Epoch 151, loss: 673.199533\n",
      "Epoch 152, loss: 674.446497\n",
      "Epoch 153, loss: 672.948283\n",
      "Epoch 154, loss: 671.780912\n",
      "Epoch 155, loss: 674.878102\n",
      "Epoch 156, loss: 674.444046\n",
      "Epoch 157, loss: 673.618680\n",
      "Epoch 158, loss: 672.590973\n",
      "Epoch 159, loss: 673.812784\n",
      "Epoch 160, loss: 671.923942\n",
      "Epoch 161, loss: 672.089838\n",
      "Epoch 162, loss: 671.782155\n",
      "Epoch 163, loss: 673.408437\n",
      "Epoch 164, loss: 674.335200\n",
      "Epoch 165, loss: 671.799493\n",
      "Epoch 166, loss: 670.852679\n",
      "Epoch 167, loss: 674.878771\n",
      "Epoch 168, loss: 673.642739\n",
      "Epoch 169, loss: 673.669836\n",
      "Epoch 170, loss: 674.213460\n",
      "Epoch 171, loss: 673.471892\n",
      "Epoch 172, loss: 673.861643\n",
      "Epoch 173, loss: 672.178181\n",
      "Epoch 174, loss: 674.707776\n",
      "Epoch 175, loss: 674.213911\n",
      "Epoch 176, loss: 671.308326\n",
      "Epoch 177, loss: 672.573536\n",
      "Epoch 178, loss: 673.399023\n",
      "Epoch 179, loss: 673.445035\n",
      "Epoch 180, loss: 670.653243\n",
      "Epoch 181, loss: 674.139409\n",
      "Epoch 182, loss: 673.065360\n",
      "Epoch 183, loss: 672.734646\n",
      "Epoch 184, loss: 675.204255\n",
      "Epoch 185, loss: 672.190440\n",
      "Epoch 186, loss: 674.427994\n",
      "Epoch 187, loss: 673.744258\n",
      "Epoch 188, loss: 670.803378\n",
      "Epoch 189, loss: 674.637658\n",
      "Epoch 190, loss: 672.354550\n",
      "Epoch 191, loss: 673.943470\n",
      "Epoch 192, loss: 672.840991\n",
      "Epoch 193, loss: 672.057609\n",
      "Epoch 194, loss: 672.039424\n",
      "Epoch 195, loss: 674.142004\n",
      "Epoch 196, loss: 670.928000\n",
      "Epoch 197, loss: 675.083217\n",
      "Epoch 198, loss: 671.947044\n",
      "Epoch 199, loss: 674.427604\n",
      "Epoch 0, loss: 690.486287\n",
      "Epoch 1, loss: 685.449455\n",
      "Epoch 2, loss: 685.447626\n",
      "Epoch 3, loss: 685.255058\n",
      "Epoch 4, loss: 682.896032\n",
      "Epoch 5, loss: 680.565011\n",
      "Epoch 6, loss: 679.516173\n",
      "Epoch 7, loss: 676.302714\n",
      "Epoch 8, loss: 677.484553\n",
      "Epoch 9, loss: 676.192139\n",
      "Epoch 10, loss: 674.922141\n",
      "Epoch 11, loss: 678.536973\n",
      "Epoch 12, loss: 675.456787\n",
      "Epoch 13, loss: 674.609598\n",
      "Epoch 14, loss: 673.011270\n",
      "Epoch 15, loss: 677.122819\n",
      "Epoch 16, loss: 674.390806\n",
      "Epoch 17, loss: 674.715096\n",
      "Epoch 18, loss: 674.430687\n",
      "Epoch 19, loss: 671.200265\n",
      "Epoch 20, loss: 674.436131\n",
      "Epoch 21, loss: 673.362089\n",
      "Epoch 22, loss: 674.049134\n",
      "Epoch 23, loss: 675.710071\n",
      "Epoch 24, loss: 674.835964\n",
      "Epoch 25, loss: 673.670497\n",
      "Epoch 26, loss: 675.453526\n",
      "Epoch 27, loss: 673.591966\n",
      "Epoch 28, loss: 674.926367\n",
      "Epoch 29, loss: 671.593287\n",
      "Epoch 30, loss: 672.628799\n",
      "Epoch 31, loss: 672.697172\n",
      "Epoch 32, loss: 673.254850\n",
      "Epoch 33, loss: 674.900876\n",
      "Epoch 34, loss: 671.251929\n",
      "Epoch 35, loss: 672.023352\n",
      "Epoch 36, loss: 676.069712\n",
      "Epoch 37, loss: 672.686483\n",
      "Epoch 38, loss: 673.336113\n",
      "Epoch 39, loss: 673.935910\n",
      "Epoch 40, loss: 673.541228\n",
      "Epoch 41, loss: 675.361905\n",
      "Epoch 42, loss: 675.327445\n",
      "Epoch 43, loss: 672.712969\n",
      "Epoch 44, loss: 674.808214\n",
      "Epoch 45, loss: 670.103483\n",
      "Epoch 46, loss: 673.212674\n",
      "Epoch 47, loss: 672.450963\n",
      "Epoch 48, loss: 673.211026\n",
      "Epoch 49, loss: 673.443350\n",
      "Epoch 50, loss: 674.547115\n",
      "Epoch 51, loss: 674.512926\n",
      "Epoch 52, loss: 672.755555\n",
      "Epoch 53, loss: 673.372838\n",
      "Epoch 54, loss: 674.803091\n",
      "Epoch 55, loss: 673.669012\n",
      "Epoch 56, loss: 674.160691\n",
      "Epoch 57, loss: 672.230879\n",
      "Epoch 58, loss: 672.572197\n",
      "Epoch 59, loss: 676.398574\n",
      "Epoch 60, loss: 672.326264\n",
      "Epoch 61, loss: 675.698382\n",
      "Epoch 62, loss: 674.170610\n",
      "Epoch 63, loss: 672.815226\n",
      "Epoch 64, loss: 676.157022\n",
      "Epoch 65, loss: 675.703110\n",
      "Epoch 66, loss: 673.285089\n",
      "Epoch 67, loss: 672.416306\n",
      "Epoch 68, loss: 673.867803\n",
      "Epoch 69, loss: 672.222534\n",
      "Epoch 70, loss: 673.001502\n",
      "Epoch 71, loss: 674.366558\n",
      "Epoch 72, loss: 672.914149\n",
      "Epoch 73, loss: 670.720562\n",
      "Epoch 74, loss: 670.895733\n",
      "Epoch 75, loss: 672.864647\n",
      "Epoch 76, loss: 674.427739\n",
      "Epoch 77, loss: 675.957287\n",
      "Epoch 78, loss: 674.361238\n",
      "Epoch 79, loss: 673.166101\n",
      "Epoch 80, loss: 672.472142\n",
      "Epoch 81, loss: 673.641986\n",
      "Epoch 82, loss: 674.159863\n",
      "Epoch 83, loss: 673.325045\n",
      "Epoch 84, loss: 675.159108\n",
      "Epoch 85, loss: 673.219941\n",
      "Epoch 86, loss: 673.377717\n",
      "Epoch 87, loss: 674.265330\n",
      "Epoch 88, loss: 671.999357\n",
      "Epoch 89, loss: 673.015321\n",
      "Epoch 90, loss: 676.327895\n",
      "Epoch 91, loss: 673.318192\n",
      "Epoch 92, loss: 673.612433\n",
      "Epoch 93, loss: 670.381322\n",
      "Epoch 94, loss: 672.007229\n",
      "Epoch 95, loss: 673.750278\n",
      "Epoch 96, loss: 672.660800\n",
      "Epoch 97, loss: 674.561925\n",
      "Epoch 98, loss: 675.543577\n",
      "Epoch 99, loss: 674.711190\n",
      "Epoch 100, loss: 673.201812\n",
      "Epoch 101, loss: 673.676350\n",
      "Epoch 102, loss: 672.155980\n",
      "Epoch 103, loss: 671.978767\n",
      "Epoch 104, loss: 671.866313\n",
      "Epoch 105, loss: 673.379766\n",
      "Epoch 106, loss: 673.901660\n",
      "Epoch 107, loss: 672.582507\n",
      "Epoch 108, loss: 672.550833\n",
      "Epoch 109, loss: 673.171868\n",
      "Epoch 110, loss: 673.402059\n",
      "Epoch 111, loss: 673.225046\n",
      "Epoch 112, loss: 673.994045\n",
      "Epoch 113, loss: 674.168345\n",
      "Epoch 114, loss: 672.414546\n",
      "Epoch 115, loss: 673.968694\n",
      "Epoch 116, loss: 672.872480\n",
      "Epoch 117, loss: 674.146543\n",
      "Epoch 118, loss: 672.332762\n",
      "Epoch 119, loss: 672.422995\n",
      "Epoch 120, loss: 673.981245\n",
      "Epoch 121, loss: 673.752021\n",
      "Epoch 122, loss: 673.250526\n",
      "Epoch 123, loss: 671.720534\n",
      "Epoch 124, loss: 674.871089\n",
      "Epoch 125, loss: 673.134501\n",
      "Epoch 126, loss: 673.961880\n",
      "Epoch 127, loss: 674.487854\n",
      "Epoch 128, loss: 670.394747\n",
      "Epoch 129, loss: 672.499307\n",
      "Epoch 130, loss: 675.740968\n",
      "Epoch 131, loss: 673.243763\n",
      "Epoch 132, loss: 674.316633\n",
      "Epoch 133, loss: 670.365406\n",
      "Epoch 134, loss: 673.629305\n",
      "Epoch 135, loss: 671.985543\n",
      "Epoch 136, loss: 674.415318\n",
      "Epoch 137, loss: 672.150551\n",
      "Epoch 138, loss: 673.322481\n",
      "Epoch 139, loss: 673.255166\n",
      "Epoch 140, loss: 674.760399\n",
      "Epoch 141, loss: 673.813735\n",
      "Epoch 142, loss: 672.836385\n",
      "Epoch 143, loss: 674.838784\n",
      "Epoch 144, loss: 672.598248\n",
      "Epoch 145, loss: 673.138020\n",
      "Epoch 146, loss: 671.332439\n",
      "Epoch 147, loss: 672.848417\n",
      "Epoch 148, loss: 674.694143\n",
      "Epoch 149, loss: 673.373690\n",
      "Epoch 150, loss: 673.279390\n",
      "Epoch 151, loss: 674.465308\n",
      "Epoch 152, loss: 672.288444\n",
      "Epoch 153, loss: 673.365725\n",
      "Epoch 154, loss: 672.088900\n",
      "Epoch 155, loss: 674.248368\n",
      "Epoch 156, loss: 674.852049\n",
      "Epoch 157, loss: 674.713505\n",
      "Epoch 158, loss: 674.215186\n",
      "Epoch 159, loss: 673.475898\n",
      "Epoch 160, loss: 674.288797\n",
      "Epoch 161, loss: 672.534928\n",
      "Epoch 162, loss: 671.737773\n",
      "Epoch 163, loss: 675.067124\n",
      "Epoch 164, loss: 672.220513\n",
      "Epoch 165, loss: 672.900476\n",
      "Epoch 166, loss: 675.281193\n",
      "Epoch 167, loss: 673.426236\n",
      "Epoch 168, loss: 672.430027\n",
      "Epoch 169, loss: 674.565601\n",
      "Epoch 170, loss: 673.117996\n",
      "Epoch 171, loss: 677.589807\n",
      "Epoch 172, loss: 674.511889\n",
      "Epoch 173, loss: 676.263936\n",
      "Epoch 174, loss: 671.611181\n",
      "Epoch 175, loss: 673.154731\n",
      "Epoch 176, loss: 671.970674\n",
      "Epoch 177, loss: 674.343821\n",
      "Epoch 178, loss: 673.184171\n",
      "Epoch 179, loss: 674.717224\n",
      "Epoch 180, loss: 674.329542\n",
      "Epoch 181, loss: 672.318207\n",
      "Epoch 182, loss: 674.503121\n",
      "Epoch 183, loss: 671.222796\n",
      "Epoch 184, loss: 675.253953\n",
      "Epoch 185, loss: 672.627315\n",
      "Epoch 186, loss: 671.998397\n",
      "Epoch 187, loss: 671.595677\n",
      "Epoch 188, loss: 673.422596\n",
      "Epoch 189, loss: 674.580315\n",
      "Epoch 190, loss: 673.896190\n",
      "Epoch 191, loss: 673.901919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192, loss: 672.818646\n",
      "Epoch 193, loss: 674.354483\n",
      "Epoch 194, loss: 673.407279\n",
      "Epoch 195, loss: 674.664027\n",
      "Epoch 196, loss: 673.111402\n",
      "Epoch 197, loss: 674.239368\n",
      "Epoch 198, loss: 673.674985\n",
      "Epoch 199, loss: 672.744234\n",
      "Epoch 0, loss: 687.830237\n",
      "Epoch 1, loss: 684.900087\n",
      "Epoch 2, loss: 685.652976\n",
      "Epoch 3, loss: 682.064890\n",
      "Epoch 4, loss: 680.371168\n",
      "Epoch 5, loss: 681.471428\n",
      "Epoch 6, loss: 678.582502\n",
      "Epoch 7, loss: 678.192235\n",
      "Epoch 8, loss: 677.775457\n",
      "Epoch 9, loss: 675.236529\n",
      "Epoch 10, loss: 676.404504\n",
      "Epoch 11, loss: 675.256499\n",
      "Epoch 12, loss: 676.222170\n",
      "Epoch 13, loss: 673.702250\n",
      "Epoch 14, loss: 675.931497\n",
      "Epoch 15, loss: 673.852425\n",
      "Epoch 16, loss: 674.490837\n",
      "Epoch 17, loss: 674.459778\n",
      "Epoch 18, loss: 674.377570\n",
      "Epoch 19, loss: 672.447721\n",
      "Epoch 20, loss: 673.613011\n",
      "Epoch 21, loss: 673.459896\n",
      "Epoch 22, loss: 673.573117\n",
      "Epoch 23, loss: 675.618525\n",
      "Epoch 24, loss: 674.714882\n",
      "Epoch 25, loss: 672.937410\n",
      "Epoch 26, loss: 674.611129\n",
      "Epoch 27, loss: 673.501006\n",
      "Epoch 28, loss: 673.539858\n",
      "Epoch 29, loss: 676.382047\n",
      "Epoch 30, loss: 674.898101\n",
      "Epoch 31, loss: 672.953483\n",
      "Epoch 32, loss: 672.296216\n",
      "Epoch 33, loss: 672.730342\n",
      "Epoch 34, loss: 672.790920\n",
      "Epoch 35, loss: 674.811656\n",
      "Epoch 36, loss: 672.430582\n",
      "Epoch 37, loss: 675.046767\n",
      "Epoch 38, loss: 674.895877\n",
      "Epoch 39, loss: 673.873858\n",
      "Epoch 40, loss: 670.613146\n",
      "Epoch 41, loss: 672.428412\n",
      "Epoch 42, loss: 674.572252\n",
      "Epoch 43, loss: 674.078897\n",
      "Epoch 44, loss: 674.430540\n",
      "Epoch 45, loss: 673.086302\n",
      "Epoch 46, loss: 675.124412\n",
      "Epoch 47, loss: 674.086875\n",
      "Epoch 48, loss: 676.277083\n",
      "Epoch 49, loss: 675.362140\n",
      "Epoch 50, loss: 674.148735\n",
      "Epoch 51, loss: 671.964779\n",
      "Epoch 52, loss: 672.592106\n",
      "Epoch 53, loss: 673.746481\n",
      "Epoch 54, loss: 675.348269\n",
      "Epoch 55, loss: 673.606687\n",
      "Epoch 56, loss: 675.039904\n",
      "Epoch 57, loss: 674.048334\n",
      "Epoch 58, loss: 671.307796\n",
      "Epoch 59, loss: 674.017345\n",
      "Epoch 60, loss: 675.588280\n",
      "Epoch 61, loss: 673.670889\n",
      "Epoch 62, loss: 672.248298\n",
      "Epoch 63, loss: 672.654788\n",
      "Epoch 64, loss: 674.778621\n",
      "Epoch 65, loss: 672.601437\n",
      "Epoch 66, loss: 673.348053\n",
      "Epoch 67, loss: 674.083888\n",
      "Epoch 68, loss: 673.840958\n",
      "Epoch 69, loss: 672.709786\n",
      "Epoch 70, loss: 674.398921\n",
      "Epoch 71, loss: 673.641576\n",
      "Epoch 72, loss: 672.530068\n",
      "Epoch 73, loss: 674.997021\n",
      "Epoch 74, loss: 675.236423\n",
      "Epoch 75, loss: 673.611366\n",
      "Epoch 76, loss: 673.266374\n",
      "Epoch 77, loss: 674.609352\n",
      "Epoch 78, loss: 672.937245\n",
      "Epoch 79, loss: 672.751100\n",
      "Epoch 80, loss: 677.274697\n",
      "Epoch 81, loss: 676.209428\n",
      "Epoch 82, loss: 674.264365\n",
      "Epoch 83, loss: 672.971743\n",
      "Epoch 84, loss: 674.646787\n",
      "Epoch 85, loss: 673.674560\n",
      "Epoch 86, loss: 672.394662\n",
      "Epoch 87, loss: 673.178560\n",
      "Epoch 88, loss: 676.003166\n",
      "Epoch 89, loss: 674.863752\n",
      "Epoch 90, loss: 676.162358\n",
      "Epoch 91, loss: 673.453194\n",
      "Epoch 92, loss: 675.268727\n",
      "Epoch 93, loss: 671.463438\n",
      "Epoch 94, loss: 674.297083\n",
      "Epoch 95, loss: 673.388876\n",
      "Epoch 96, loss: 674.646781\n",
      "Epoch 97, loss: 672.099953\n",
      "Epoch 98, loss: 674.630454\n",
      "Epoch 99, loss: 673.831443\n",
      "Epoch 100, loss: 674.027189\n",
      "Epoch 101, loss: 674.113822\n",
      "Epoch 102, loss: 671.214270\n",
      "Epoch 103, loss: 676.050809\n",
      "Epoch 104, loss: 673.436243\n",
      "Epoch 105, loss: 675.707350\n",
      "Epoch 106, loss: 673.486525\n",
      "Epoch 107, loss: 672.246607\n",
      "Epoch 108, loss: 672.081092\n",
      "Epoch 109, loss: 673.558784\n",
      "Epoch 110, loss: 674.828099\n",
      "Epoch 111, loss: 673.444942\n",
      "Epoch 112, loss: 670.814821\n",
      "Epoch 113, loss: 673.088198\n",
      "Epoch 114, loss: 673.649023\n",
      "Epoch 115, loss: 673.450528\n",
      "Epoch 116, loss: 672.510225\n",
      "Epoch 117, loss: 675.962909\n",
      "Epoch 118, loss: 672.127625\n",
      "Epoch 119, loss: 672.489757\n",
      "Epoch 120, loss: 675.472612\n",
      "Epoch 121, loss: 672.729706\n",
      "Epoch 122, loss: 674.288919\n",
      "Epoch 123, loss: 674.077564\n",
      "Epoch 124, loss: 672.070598\n",
      "Epoch 125, loss: 669.962735\n",
      "Epoch 126, loss: 678.049664\n",
      "Epoch 127, loss: 672.160119\n",
      "Epoch 128, loss: 671.844076\n",
      "Epoch 129, loss: 675.708578\n",
      "Epoch 130, loss: 675.609060\n",
      "Epoch 131, loss: 673.864673\n",
      "Epoch 132, loss: 672.717866\n",
      "Epoch 133, loss: 673.717158\n",
      "Epoch 134, loss: 670.243965\n",
      "Epoch 135, loss: 672.208324\n",
      "Epoch 136, loss: 671.791103\n",
      "Epoch 137, loss: 675.155413\n",
      "Epoch 138, loss: 673.303868\n",
      "Epoch 139, loss: 673.214640\n",
      "Epoch 140, loss: 672.570630\n",
      "Epoch 141, loss: 672.687466\n",
      "Epoch 142, loss: 673.621544\n",
      "Epoch 143, loss: 674.651274\n",
      "Epoch 144, loss: 674.148575\n",
      "Epoch 145, loss: 674.706128\n",
      "Epoch 146, loss: 672.277892\n",
      "Epoch 147, loss: 673.310946\n",
      "Epoch 148, loss: 675.363611\n",
      "Epoch 149, loss: 673.396997\n",
      "Epoch 150, loss: 672.987922\n",
      "Epoch 151, loss: 674.087588\n",
      "Epoch 152, loss: 672.832275\n",
      "Epoch 153, loss: 675.453411\n",
      "Epoch 154, loss: 672.802331\n",
      "Epoch 155, loss: 674.207545\n",
      "Epoch 156, loss: 675.012789\n",
      "Epoch 157, loss: 673.063783\n",
      "Epoch 158, loss: 671.733124\n",
      "Epoch 159, loss: 672.826084\n",
      "Epoch 160, loss: 674.012664\n",
      "Epoch 161, loss: 674.805417\n",
      "Epoch 162, loss: 674.320744\n",
      "Epoch 163, loss: 672.772942\n",
      "Epoch 164, loss: 675.087531\n",
      "Epoch 165, loss: 673.632530\n",
      "Epoch 166, loss: 673.718051\n",
      "Epoch 167, loss: 671.696648\n",
      "Epoch 168, loss: 672.961860\n",
      "Epoch 169, loss: 673.578081\n",
      "Epoch 170, loss: 672.573941\n",
      "Epoch 171, loss: 674.964145\n",
      "Epoch 172, loss: 672.710886\n",
      "Epoch 173, loss: 674.861804\n",
      "Epoch 174, loss: 674.335255\n",
      "Epoch 175, loss: 673.826474\n",
      "Epoch 176, loss: 672.474215\n",
      "Epoch 177, loss: 673.866997\n",
      "Epoch 178, loss: 673.288881\n",
      "Epoch 179, loss: 673.683046\n",
      "Epoch 180, loss: 674.924691\n",
      "Epoch 181, loss: 673.699617\n",
      "Epoch 182, loss: 674.316539\n",
      "Epoch 183, loss: 674.520328\n",
      "Epoch 184, loss: 673.089356\n",
      "Epoch 185, loss: 673.170715\n",
      "Epoch 186, loss: 675.836249\n",
      "Epoch 187, loss: 671.361411\n",
      "Epoch 188, loss: 673.266459\n",
      "Epoch 189, loss: 670.871412\n",
      "Epoch 190, loss: 672.419299\n",
      "Epoch 191, loss: 674.655119\n",
      "Epoch 192, loss: 673.654122\n",
      "Epoch 193, loss: 672.856386\n",
      "Epoch 194, loss: 673.853666\n",
      "Epoch 195, loss: 673.711368\n",
      "Epoch 196, loss: 673.601891\n",
      "Epoch 197, loss: 674.256818\n",
      "Epoch 198, loss: 676.466733\n",
      "Epoch 199, loss: 671.707050\n",
      "Epoch 0, loss: 686.984494\n",
      "Epoch 1, loss: 687.358500\n",
      "Epoch 2, loss: 682.904994\n",
      "Epoch 3, loss: 683.353637\n",
      "Epoch 4, loss: 680.895380\n",
      "Epoch 5, loss: 682.573025\n",
      "Epoch 6, loss: 679.293785\n",
      "Epoch 7, loss: 675.269891\n",
      "Epoch 8, loss: 676.479711\n",
      "Epoch 9, loss: 676.165048\n",
      "Epoch 10, loss: 676.236373\n",
      "Epoch 11, loss: 674.636684\n",
      "Epoch 12, loss: 675.753542\n",
      "Epoch 13, loss: 673.545545\n",
      "Epoch 14, loss: 672.769254\n",
      "Epoch 15, loss: 673.507776\n",
      "Epoch 16, loss: 674.038125\n",
      "Epoch 17, loss: 674.812741\n",
      "Epoch 18, loss: 673.830438\n",
      "Epoch 19, loss: 675.128120\n",
      "Epoch 20, loss: 673.108385\n",
      "Epoch 21, loss: 674.454761\n",
      "Epoch 22, loss: 673.415745\n",
      "Epoch 23, loss: 674.992882\n",
      "Epoch 24, loss: 674.680470\n",
      "Epoch 25, loss: 673.756879\n",
      "Epoch 26, loss: 677.452973\n",
      "Epoch 27, loss: 672.919490\n",
      "Epoch 28, loss: 674.098607\n",
      "Epoch 29, loss: 671.234040\n",
      "Epoch 30, loss: 675.028022\n",
      "Epoch 31, loss: 673.068961\n",
      "Epoch 32, loss: 673.021710\n",
      "Epoch 33, loss: 672.966814\n",
      "Epoch 34, loss: 674.916216\n",
      "Epoch 35, loss: 671.534991\n",
      "Epoch 36, loss: 671.280038\n",
      "Epoch 37, loss: 675.206391\n",
      "Epoch 38, loss: 674.939168\n",
      "Epoch 39, loss: 676.066533\n",
      "Epoch 40, loss: 673.861479\n",
      "Epoch 41, loss: 674.030279\n",
      "Epoch 42, loss: 673.887998\n",
      "Epoch 43, loss: 673.516399\n",
      "Epoch 44, loss: 676.644219\n",
      "Epoch 45, loss: 671.665764\n",
      "Epoch 46, loss: 674.700376\n",
      "Epoch 47, loss: 673.144552\n",
      "Epoch 48, loss: 674.483271\n",
      "Epoch 49, loss: 673.431974\n",
      "Epoch 50, loss: 674.547534\n",
      "Epoch 51, loss: 673.933521\n",
      "Epoch 52, loss: 673.189360\n",
      "Epoch 53, loss: 672.685430\n",
      "Epoch 54, loss: 674.115437\n",
      "Epoch 55, loss: 676.361576\n",
      "Epoch 56, loss: 674.519331\n",
      "Epoch 57, loss: 671.387095\n",
      "Epoch 58, loss: 671.565864\n",
      "Epoch 59, loss: 674.638208\n",
      "Epoch 60, loss: 672.105970\n",
      "Epoch 61, loss: 673.403170\n",
      "Epoch 62, loss: 675.670830\n",
      "Epoch 63, loss: 673.073568\n",
      "Epoch 64, loss: 673.759900\n",
      "Epoch 65, loss: 674.268866\n",
      "Epoch 66, loss: 675.118170\n",
      "Epoch 67, loss: 673.236478\n",
      "Epoch 68, loss: 672.889344\n",
      "Epoch 69, loss: 675.076904\n",
      "Epoch 70, loss: 672.300284\n",
      "Epoch 71, loss: 674.009781\n",
      "Epoch 72, loss: 672.926670\n",
      "Epoch 73, loss: 674.507421\n",
      "Epoch 74, loss: 674.429244\n",
      "Epoch 75, loss: 674.391587\n",
      "Epoch 76, loss: 670.432550\n",
      "Epoch 77, loss: 673.614795\n",
      "Epoch 78, loss: 668.937866\n",
      "Epoch 79, loss: 672.441734\n",
      "Epoch 80, loss: 673.314850\n",
      "Epoch 81, loss: 672.350309\n",
      "Epoch 82, loss: 675.329267\n",
      "Epoch 83, loss: 673.310067\n",
      "Epoch 84, loss: 672.831805\n",
      "Epoch 85, loss: 672.304833\n",
      "Epoch 86, loss: 675.082378\n",
      "Epoch 87, loss: 673.326388\n",
      "Epoch 88, loss: 672.970085\n",
      "Epoch 89, loss: 672.170207\n",
      "Epoch 90, loss: 672.408609\n",
      "Epoch 91, loss: 675.297553\n",
      "Epoch 92, loss: 673.051388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93, loss: 673.990621\n",
      "Epoch 94, loss: 670.940928\n",
      "Epoch 95, loss: 674.176676\n",
      "Epoch 96, loss: 673.058717\n",
      "Epoch 97, loss: 672.132565\n",
      "Epoch 98, loss: 671.840864\n",
      "Epoch 99, loss: 674.390544\n",
      "Epoch 100, loss: 672.109520\n",
      "Epoch 101, loss: 673.887469\n",
      "Epoch 102, loss: 672.746897\n",
      "Epoch 103, loss: 674.107027\n",
      "Epoch 104, loss: 674.446046\n",
      "Epoch 105, loss: 672.663813\n",
      "Epoch 106, loss: 674.884128\n",
      "Epoch 107, loss: 671.404141\n",
      "Epoch 108, loss: 673.236046\n",
      "Epoch 109, loss: 674.757390\n",
      "Epoch 110, loss: 674.923655\n",
      "Epoch 111, loss: 673.702252\n",
      "Epoch 112, loss: 674.757293\n",
      "Epoch 113, loss: 672.992491\n",
      "Epoch 114, loss: 675.072537\n",
      "Epoch 115, loss: 672.930148\n",
      "Epoch 116, loss: 673.098600\n",
      "Epoch 117, loss: 673.322190\n",
      "Epoch 118, loss: 672.068656\n",
      "Epoch 119, loss: 675.647881\n",
      "Epoch 120, loss: 671.574007\n",
      "Epoch 121, loss: 673.263885\n",
      "Epoch 122, loss: 670.721448\n",
      "Epoch 123, loss: 671.874772\n",
      "Epoch 124, loss: 673.477892\n",
      "Epoch 125, loss: 672.214841\n",
      "Epoch 126, loss: 673.287146\n",
      "Epoch 127, loss: 674.412848\n",
      "Epoch 128, loss: 674.288577\n",
      "Epoch 129, loss: 672.542579\n",
      "Epoch 130, loss: 672.566526\n",
      "Epoch 131, loss: 673.715983\n",
      "Epoch 132, loss: 674.278455\n",
      "Epoch 133, loss: 677.094427\n",
      "Epoch 134, loss: 673.220271\n",
      "Epoch 135, loss: 673.510402\n",
      "Epoch 136, loss: 675.714903\n",
      "Epoch 137, loss: 672.580016\n",
      "Epoch 138, loss: 672.779607\n",
      "Epoch 139, loss: 672.725995\n",
      "Epoch 140, loss: 676.800763\n",
      "Epoch 141, loss: 673.012453\n",
      "Epoch 142, loss: 669.737797\n",
      "Epoch 143, loss: 671.850689\n",
      "Epoch 144, loss: 673.797798\n",
      "Epoch 145, loss: 673.448552\n",
      "Epoch 146, loss: 675.060324\n",
      "Epoch 147, loss: 672.856344\n",
      "Epoch 148, loss: 673.914224\n",
      "Epoch 149, loss: 671.986535\n",
      "Epoch 150, loss: 671.587797\n",
      "Epoch 151, loss: 674.243751\n",
      "Epoch 152, loss: 673.566495\n",
      "Epoch 153, loss: 672.452639\n",
      "Epoch 154, loss: 674.148261\n",
      "Epoch 155, loss: 673.380348\n",
      "Epoch 156, loss: 673.767719\n",
      "Epoch 157, loss: 674.583543\n",
      "Epoch 158, loss: 672.482047\n",
      "Epoch 159, loss: 673.017842\n",
      "Epoch 160, loss: 673.365504\n",
      "Epoch 161, loss: 672.586458\n",
      "Epoch 162, loss: 673.714985\n",
      "Epoch 163, loss: 675.299021\n",
      "Epoch 164, loss: 674.437181\n",
      "Epoch 165, loss: 672.532046\n",
      "Epoch 166, loss: 671.522532\n",
      "Epoch 167, loss: 674.657431\n",
      "Epoch 168, loss: 674.155227\n",
      "Epoch 169, loss: 673.432316\n",
      "Epoch 170, loss: 673.698685\n",
      "Epoch 171, loss: 672.636720\n",
      "Epoch 172, loss: 671.822845\n",
      "Epoch 173, loss: 672.950356\n",
      "Epoch 174, loss: 673.372652\n",
      "Epoch 175, loss: 673.122838\n",
      "Epoch 176, loss: 674.725459\n",
      "Epoch 177, loss: 673.494286\n",
      "Epoch 178, loss: 674.953356\n",
      "Epoch 179, loss: 672.796486\n",
      "Epoch 180, loss: 677.310254\n",
      "Epoch 181, loss: 674.405301\n",
      "Epoch 182, loss: 671.506049\n",
      "Epoch 183, loss: 674.365831\n",
      "Epoch 184, loss: 672.251050\n",
      "Epoch 185, loss: 674.792709\n",
      "Epoch 186, loss: 674.150405\n",
      "Epoch 187, loss: 672.414250\n",
      "Epoch 188, loss: 674.069526\n",
      "Epoch 189, loss: 672.245028\n",
      "Epoch 190, loss: 671.720586\n",
      "Epoch 191, loss: 672.613763\n",
      "Epoch 192, loss: 674.771912\n",
      "Epoch 193, loss: 675.527247\n",
      "Epoch 194, loss: 675.481132\n",
      "Epoch 195, loss: 672.895380\n",
      "Epoch 196, loss: 675.444206\n",
      "Epoch 197, loss: 674.971914\n",
      "Epoch 198, loss: 672.622602\n",
      "Epoch 199, loss: 673.731133\n",
      "Epoch 0, loss: 690.509893\n",
      "Epoch 1, loss: 689.876635\n",
      "Epoch 2, loss: 689.514914\n",
      "Epoch 3, loss: 689.348383\n",
      "Epoch 4, loss: 689.826660\n",
      "Epoch 5, loss: 689.145610\n",
      "Epoch 6, loss: 688.693013\n",
      "Epoch 7, loss: 689.514976\n",
      "Epoch 8, loss: 688.153544\n",
      "Epoch 9, loss: 687.584384\n",
      "Epoch 10, loss: 687.458483\n",
      "Epoch 11, loss: 687.155229\n",
      "Epoch 12, loss: 686.859597\n",
      "Epoch 13, loss: 686.550937\n",
      "Epoch 14, loss: 685.797139\n",
      "Epoch 15, loss: 686.026604\n",
      "Epoch 16, loss: 686.337308\n",
      "Epoch 17, loss: 685.323475\n",
      "Epoch 18, loss: 685.358334\n",
      "Epoch 19, loss: 685.034043\n",
      "Epoch 20, loss: 684.569657\n",
      "Epoch 21, loss: 684.433067\n",
      "Epoch 22, loss: 684.887506\n",
      "Epoch 23, loss: 684.572467\n",
      "Epoch 24, loss: 683.806633\n",
      "Epoch 25, loss: 683.728575\n",
      "Epoch 26, loss: 684.228533\n",
      "Epoch 27, loss: 684.305583\n",
      "Epoch 28, loss: 682.346630\n",
      "Epoch 29, loss: 683.037124\n",
      "Epoch 30, loss: 682.721065\n",
      "Epoch 31, loss: 683.338721\n",
      "Epoch 32, loss: 682.540178\n",
      "Epoch 33, loss: 682.160727\n",
      "Epoch 34, loss: 682.493525\n",
      "Epoch 35, loss: 682.075904\n",
      "Epoch 36, loss: 681.363995\n",
      "Epoch 37, loss: 681.178990\n",
      "Epoch 38, loss: 681.325170\n",
      "Epoch 39, loss: 681.222475\n",
      "Epoch 40, loss: 680.925788\n",
      "Epoch 41, loss: 680.795783\n",
      "Epoch 42, loss: 680.214060\n",
      "Epoch 43, loss: 680.917314\n",
      "Epoch 44, loss: 681.030083\n",
      "Epoch 45, loss: 680.179806\n",
      "Epoch 46, loss: 679.716200\n",
      "Epoch 47, loss: 680.024522\n",
      "Epoch 48, loss: 680.608419\n",
      "Epoch 49, loss: 680.198541\n",
      "Epoch 50, loss: 679.809044\n",
      "Epoch 51, loss: 679.611551\n",
      "Epoch 52, loss: 679.486792\n",
      "Epoch 53, loss: 678.961221\n",
      "Epoch 54, loss: 679.210581\n",
      "Epoch 55, loss: 679.638732\n",
      "Epoch 56, loss: 678.765763\n",
      "Epoch 57, loss: 678.543966\n",
      "Epoch 58, loss: 678.819073\n",
      "Epoch 59, loss: 678.256138\n",
      "Epoch 60, loss: 678.639318\n",
      "Epoch 61, loss: 678.293543\n",
      "Epoch 62, loss: 678.324295\n",
      "Epoch 63, loss: 677.721539\n",
      "Epoch 64, loss: 678.074683\n",
      "Epoch 65, loss: 677.813719\n",
      "Epoch 66, loss: 678.450441\n",
      "Epoch 67, loss: 678.227628\n",
      "Epoch 68, loss: 678.265092\n",
      "Epoch 69, loss: 677.517185\n",
      "Epoch 70, loss: 677.310553\n",
      "Epoch 71, loss: 677.388238\n",
      "Epoch 72, loss: 677.592558\n",
      "Epoch 73, loss: 677.699379\n",
      "Epoch 74, loss: 677.887397\n",
      "Epoch 75, loss: 677.459600\n",
      "Epoch 76, loss: 677.048714\n",
      "Epoch 77, loss: 676.620022\n",
      "Epoch 78, loss: 677.227867\n",
      "Epoch 79, loss: 676.749480\n",
      "Epoch 80, loss: 676.638780\n",
      "Epoch 81, loss: 676.617220\n",
      "Epoch 82, loss: 676.791871\n",
      "Epoch 83, loss: 676.583195\n",
      "Epoch 84, loss: 676.226092\n",
      "Epoch 85, loss: 676.797324\n",
      "Epoch 86, loss: 676.378997\n",
      "Epoch 87, loss: 676.631481\n",
      "Epoch 88, loss: 676.444525\n",
      "Epoch 89, loss: 676.228226\n",
      "Epoch 90, loss: 676.256601\n",
      "Epoch 91, loss: 676.572513\n",
      "Epoch 92, loss: 675.943794\n",
      "Epoch 93, loss: 675.913497\n",
      "Epoch 94, loss: 676.755780\n",
      "Epoch 95, loss: 675.716629\n",
      "Epoch 96, loss: 675.510110\n",
      "Epoch 97, loss: 676.028640\n",
      "Epoch 98, loss: 675.756941\n",
      "Epoch 99, loss: 675.525605\n",
      "Epoch 100, loss: 675.597231\n",
      "Epoch 101, loss: 675.413310\n",
      "Epoch 102, loss: 675.019453\n",
      "Epoch 103, loss: 675.042739\n",
      "Epoch 104, loss: 674.954877\n",
      "Epoch 105, loss: 675.335587\n",
      "Epoch 106, loss: 674.928971\n",
      "Epoch 107, loss: 674.987515\n",
      "Epoch 108, loss: 674.962917\n",
      "Epoch 109, loss: 675.087916\n",
      "Epoch 110, loss: 675.436020\n",
      "Epoch 111, loss: 675.062818\n",
      "Epoch 112, loss: 674.987300\n",
      "Epoch 113, loss: 674.997841\n",
      "Epoch 114, loss: 674.474840\n",
      "Epoch 115, loss: 675.211219\n",
      "Epoch 116, loss: 675.124401\n",
      "Epoch 117, loss: 674.739376\n",
      "Epoch 118, loss: 674.907353\n",
      "Epoch 119, loss: 675.554387\n",
      "Epoch 120, loss: 673.623532\n",
      "Epoch 121, loss: 674.976971\n",
      "Epoch 122, loss: 673.855119\n",
      "Epoch 123, loss: 674.991436\n",
      "Epoch 124, loss: 674.735612\n",
      "Epoch 125, loss: 674.574831\n",
      "Epoch 126, loss: 674.880310\n",
      "Epoch 127, loss: 674.811469\n",
      "Epoch 128, loss: 674.677478\n",
      "Epoch 129, loss: 673.848080\n",
      "Epoch 130, loss: 674.652718\n",
      "Epoch 131, loss: 674.287814\n",
      "Epoch 132, loss: 674.636212\n",
      "Epoch 133, loss: 674.182217\n",
      "Epoch 134, loss: 673.460282\n",
      "Epoch 135, loss: 673.561785\n",
      "Epoch 136, loss: 674.190898\n",
      "Epoch 137, loss: 674.398046\n",
      "Epoch 138, loss: 674.109966\n",
      "Epoch 139, loss: 674.244238\n",
      "Epoch 140, loss: 674.528172\n",
      "Epoch 141, loss: 673.510802\n",
      "Epoch 142, loss: 674.729714\n",
      "Epoch 143, loss: 674.390762\n",
      "Epoch 144, loss: 674.339950\n",
      "Epoch 145, loss: 674.404360\n",
      "Epoch 146, loss: 673.965426\n",
      "Epoch 147, loss: 673.710794\n",
      "Epoch 148, loss: 673.975567\n",
      "Epoch 149, loss: 673.929554\n",
      "Epoch 150, loss: 674.046840\n",
      "Epoch 151, loss: 674.331970\n",
      "Epoch 152, loss: 673.198545\n",
      "Epoch 153, loss: 673.559511\n",
      "Epoch 154, loss: 673.983545\n",
      "Epoch 155, loss: 673.182647\n",
      "Epoch 156, loss: 674.091035\n",
      "Epoch 157, loss: 673.351698\n",
      "Epoch 158, loss: 673.791226\n",
      "Epoch 159, loss: 673.850648\n",
      "Epoch 160, loss: 673.729546\n",
      "Epoch 161, loss: 673.762489\n",
      "Epoch 162, loss: 673.195777\n",
      "Epoch 163, loss: 674.168501\n",
      "Epoch 164, loss: 672.994823\n",
      "Epoch 165, loss: 674.093587\n",
      "Epoch 166, loss: 673.489607\n",
      "Epoch 167, loss: 673.781770\n",
      "Epoch 168, loss: 673.784959\n",
      "Epoch 169, loss: 673.859005\n",
      "Epoch 170, loss: 673.964784\n",
      "Epoch 171, loss: 673.737673\n",
      "Epoch 172, loss: 673.618843\n",
      "Epoch 173, loss: 673.749566\n",
      "Epoch 174, loss: 673.561208\n",
      "Epoch 175, loss: 673.607100\n",
      "Epoch 176, loss: 673.749143\n",
      "Epoch 177, loss: 674.121415\n",
      "Epoch 178, loss: 673.446138\n",
      "Epoch 179, loss: 673.698044\n",
      "Epoch 180, loss: 674.082203\n",
      "Epoch 181, loss: 673.385618\n",
      "Epoch 182, loss: 673.189087\n",
      "Epoch 183, loss: 673.430848\n",
      "Epoch 184, loss: 673.210657\n",
      "Epoch 185, loss: 673.915404\n",
      "Epoch 186, loss: 673.505818\n",
      "Epoch 187, loss: 673.268852\n",
      "Epoch 188, loss: 673.286445\n",
      "Epoch 189, loss: 673.654938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190, loss: 673.184881\n",
      "Epoch 191, loss: 672.967273\n",
      "Epoch 192, loss: 672.802317\n",
      "Epoch 193, loss: 673.662136\n",
      "Epoch 194, loss: 673.256887\n",
      "Epoch 195, loss: 673.389605\n",
      "Epoch 196, loss: 673.917125\n",
      "Epoch 197, loss: 673.092732\n",
      "Epoch 198, loss: 673.806752\n",
      "Epoch 199, loss: 672.951260\n",
      "Epoch 0, loss: 691.626928\n",
      "Epoch 1, loss: 690.433623\n",
      "Epoch 2, loss: 689.768886\n",
      "Epoch 3, loss: 689.545007\n",
      "Epoch 4, loss: 689.653146\n",
      "Epoch 5, loss: 688.987700\n",
      "Epoch 6, loss: 688.408183\n",
      "Epoch 7, loss: 688.143171\n",
      "Epoch 8, loss: 688.508994\n",
      "Epoch 9, loss: 687.822513\n",
      "Epoch 10, loss: 687.904963\n",
      "Epoch 11, loss: 686.973174\n",
      "Epoch 12, loss: 686.618894\n",
      "Epoch 13, loss: 686.519575\n",
      "Epoch 14, loss: 686.787765\n",
      "Epoch 15, loss: 686.747103\n",
      "Epoch 16, loss: 684.654948\n",
      "Epoch 17, loss: 685.593237\n",
      "Epoch 18, loss: 685.211354\n",
      "Epoch 19, loss: 686.014717\n",
      "Epoch 20, loss: 685.775280\n",
      "Epoch 21, loss: 684.255874\n",
      "Epoch 22, loss: 684.218456\n",
      "Epoch 23, loss: 684.033079\n",
      "Epoch 24, loss: 684.336922\n",
      "Epoch 25, loss: 683.320036\n",
      "Epoch 26, loss: 684.508838\n",
      "Epoch 27, loss: 684.027184\n",
      "Epoch 28, loss: 683.232261\n",
      "Epoch 29, loss: 683.465693\n",
      "Epoch 30, loss: 682.939308\n",
      "Epoch 31, loss: 682.876517\n",
      "Epoch 32, loss: 682.542054\n",
      "Epoch 33, loss: 682.689926\n",
      "Epoch 34, loss: 682.116108\n",
      "Epoch 35, loss: 681.098758\n",
      "Epoch 36, loss: 682.251554\n",
      "Epoch 37, loss: 681.152363\n",
      "Epoch 38, loss: 681.879615\n",
      "Epoch 39, loss: 681.421842\n",
      "Epoch 40, loss: 681.431462\n",
      "Epoch 41, loss: 681.035665\n",
      "Epoch 42, loss: 680.642246\n",
      "Epoch 43, loss: 680.311839\n",
      "Epoch 44, loss: 680.586724\n",
      "Epoch 45, loss: 680.664101\n",
      "Epoch 46, loss: 680.538427\n",
      "Epoch 47, loss: 679.437002\n",
      "Epoch 48, loss: 680.545839\n",
      "Epoch 49, loss: 679.755443\n",
      "Epoch 50, loss: 679.950971\n",
      "Epoch 51, loss: 679.078894\n",
      "Epoch 52, loss: 679.512291\n",
      "Epoch 53, loss: 679.619028\n",
      "Epoch 54, loss: 679.263554\n",
      "Epoch 55, loss: 679.444677\n",
      "Epoch 56, loss: 678.476512\n",
      "Epoch 57, loss: 678.627348\n",
      "Epoch 58, loss: 679.116416\n",
      "Epoch 59, loss: 678.795339\n",
      "Epoch 60, loss: 678.756692\n",
      "Epoch 61, loss: 679.127000\n",
      "Epoch 62, loss: 678.074840\n",
      "Epoch 63, loss: 678.084383\n",
      "Epoch 64, loss: 678.303008\n",
      "Epoch 65, loss: 677.829105\n",
      "Epoch 66, loss: 678.609098\n",
      "Epoch 67, loss: 677.901535\n",
      "Epoch 68, loss: 676.531232\n",
      "Epoch 69, loss: 678.336996\n",
      "Epoch 70, loss: 677.739062\n",
      "Epoch 71, loss: 677.600741\n",
      "Epoch 72, loss: 676.408517\n",
      "Epoch 73, loss: 677.920065\n",
      "Epoch 74, loss: 676.879556\n",
      "Epoch 75, loss: 677.129048\n",
      "Epoch 76, loss: 676.672975\n",
      "Epoch 77, loss: 677.665236\n",
      "Epoch 78, loss: 676.585826\n",
      "Epoch 79, loss: 676.995451\n",
      "Epoch 80, loss: 676.447804\n",
      "Epoch 81, loss: 676.872197\n",
      "Epoch 82, loss: 676.815004\n",
      "Epoch 83, loss: 677.122339\n",
      "Epoch 84, loss: 676.050925\n",
      "Epoch 85, loss: 676.507874\n",
      "Epoch 86, loss: 676.379379\n",
      "Epoch 87, loss: 676.631317\n",
      "Epoch 88, loss: 675.772704\n",
      "Epoch 89, loss: 676.849793\n",
      "Epoch 90, loss: 675.341066\n",
      "Epoch 91, loss: 675.951830\n",
      "Epoch 92, loss: 675.981672\n",
      "Epoch 93, loss: 675.141648\n",
      "Epoch 94, loss: 675.790458\n",
      "Epoch 95, loss: 675.701658\n",
      "Epoch 96, loss: 676.314494\n",
      "Epoch 97, loss: 675.432273\n",
      "Epoch 98, loss: 675.769471\n",
      "Epoch 99, loss: 675.428002\n",
      "Epoch 100, loss: 675.419553\n",
      "Epoch 101, loss: 675.531517\n",
      "Epoch 102, loss: 676.005674\n",
      "Epoch 103, loss: 675.808859\n",
      "Epoch 104, loss: 675.562162\n",
      "Epoch 105, loss: 675.073391\n",
      "Epoch 106, loss: 675.563541\n",
      "Epoch 107, loss: 675.361245\n",
      "Epoch 108, loss: 675.318727\n",
      "Epoch 109, loss: 675.227025\n",
      "Epoch 110, loss: 675.369473\n",
      "Epoch 111, loss: 675.182323\n",
      "Epoch 112, loss: 674.587282\n",
      "Epoch 113, loss: 674.888698\n",
      "Epoch 114, loss: 675.389902\n",
      "Epoch 115, loss: 674.453327\n",
      "Epoch 116, loss: 674.940026\n",
      "Epoch 117, loss: 674.883871\n",
      "Epoch 118, loss: 674.940027\n",
      "Epoch 119, loss: 674.792443\n",
      "Epoch 120, loss: 674.685620\n",
      "Epoch 121, loss: 675.037747\n",
      "Epoch 122, loss: 674.381699\n",
      "Epoch 123, loss: 674.831023\n",
      "Epoch 124, loss: 674.606962\n",
      "Epoch 125, loss: 674.586041\n",
      "Epoch 126, loss: 674.041221\n",
      "Epoch 127, loss: 673.892250\n",
      "Epoch 128, loss: 674.441701\n",
      "Epoch 129, loss: 674.709500\n",
      "Epoch 130, loss: 674.200478\n",
      "Epoch 131, loss: 674.398617\n",
      "Epoch 132, loss: 674.853774\n",
      "Epoch 133, loss: 674.632983\n",
      "Epoch 134, loss: 674.276510\n",
      "Epoch 135, loss: 673.880858\n",
      "Epoch 136, loss: 673.950309\n",
      "Epoch 137, loss: 673.956201\n",
      "Epoch 138, loss: 674.042338\n",
      "Epoch 139, loss: 674.816138\n",
      "Epoch 140, loss: 673.677266\n",
      "Epoch 141, loss: 674.232925\n",
      "Epoch 142, loss: 674.583481\n",
      "Epoch 143, loss: 673.667319\n",
      "Epoch 144, loss: 673.327750\n",
      "Epoch 145, loss: 674.232950\n",
      "Epoch 146, loss: 674.026915\n",
      "Epoch 147, loss: 674.389003\n",
      "Epoch 148, loss: 674.199538\n",
      "Epoch 149, loss: 673.890619\n",
      "Epoch 150, loss: 674.245360\n",
      "Epoch 151, loss: 673.658518\n",
      "Epoch 152, loss: 673.984883\n",
      "Epoch 153, loss: 673.310713\n",
      "Epoch 154, loss: 673.787138\n",
      "Epoch 155, loss: 673.972567\n",
      "Epoch 156, loss: 674.014350\n",
      "Epoch 157, loss: 673.789165\n",
      "Epoch 158, loss: 673.543812\n",
      "Epoch 159, loss: 673.142927\n",
      "Epoch 160, loss: 673.643987\n",
      "Epoch 161, loss: 673.550653\n",
      "Epoch 162, loss: 673.515733\n",
      "Epoch 163, loss: 674.026165\n",
      "Epoch 164, loss: 673.812308\n",
      "Epoch 165, loss: 674.082207\n",
      "Epoch 166, loss: 673.365995\n",
      "Epoch 167, loss: 673.673365\n",
      "Epoch 168, loss: 674.089382\n",
      "Epoch 169, loss: 674.308459\n",
      "Epoch 170, loss: 673.653085\n",
      "Epoch 171, loss: 673.850602\n",
      "Epoch 172, loss: 673.315748\n",
      "Epoch 173, loss: 673.790660\n",
      "Epoch 174, loss: 673.149401\n",
      "Epoch 175, loss: 673.716246\n",
      "Epoch 176, loss: 673.338170\n",
      "Epoch 177, loss: 673.278689\n",
      "Epoch 178, loss: 673.383453\n",
      "Epoch 179, loss: 673.519590\n",
      "Epoch 180, loss: 673.489230\n",
      "Epoch 181, loss: 673.135274\n",
      "Epoch 182, loss: 673.346473\n",
      "Epoch 183, loss: 673.940906\n",
      "Epoch 184, loss: 673.505162\n",
      "Epoch 185, loss: 672.329584\n",
      "Epoch 186, loss: 673.271501\n",
      "Epoch 187, loss: 672.700997\n",
      "Epoch 188, loss: 674.098406\n",
      "Epoch 189, loss: 673.687069\n",
      "Epoch 190, loss: 672.638834\n",
      "Epoch 191, loss: 673.872841\n",
      "Epoch 192, loss: 673.705983\n",
      "Epoch 193, loss: 673.091423\n",
      "Epoch 194, loss: 673.393591\n",
      "Epoch 195, loss: 673.424249\n",
      "Epoch 196, loss: 673.566397\n",
      "Epoch 197, loss: 673.418658\n",
      "Epoch 198, loss: 673.878304\n",
      "Epoch 199, loss: 673.518974\n",
      "Epoch 0, loss: 690.469382\n",
      "Epoch 1, loss: 690.157052\n",
      "Epoch 2, loss: 689.897353\n",
      "Epoch 3, loss: 689.529802\n",
      "Epoch 4, loss: 689.551801\n",
      "Epoch 5, loss: 689.359311\n",
      "Epoch 6, loss: 688.281463\n",
      "Epoch 7, loss: 687.862202\n",
      "Epoch 8, loss: 687.749647\n",
      "Epoch 9, loss: 687.956220\n",
      "Epoch 10, loss: 687.816949\n",
      "Epoch 11, loss: 687.424721\n",
      "Epoch 12, loss: 687.002488\n",
      "Epoch 13, loss: 686.400670\n",
      "Epoch 14, loss: 686.679261\n",
      "Epoch 15, loss: 686.915936\n",
      "Epoch 16, loss: 685.650793\n",
      "Epoch 17, loss: 685.346625\n",
      "Epoch 18, loss: 685.113254\n",
      "Epoch 19, loss: 684.920735\n",
      "Epoch 20, loss: 684.536150\n",
      "Epoch 21, loss: 684.229769\n",
      "Epoch 22, loss: 684.526537\n",
      "Epoch 23, loss: 683.876182\n",
      "Epoch 24, loss: 684.588159\n",
      "Epoch 25, loss: 683.782798\n",
      "Epoch 26, loss: 683.310861\n",
      "Epoch 27, loss: 683.246093\n",
      "Epoch 28, loss: 683.238238\n",
      "Epoch 29, loss: 682.450506\n",
      "Epoch 30, loss: 682.898146\n",
      "Epoch 31, loss: 682.645578\n",
      "Epoch 32, loss: 682.804001\n",
      "Epoch 33, loss: 682.799172\n",
      "Epoch 34, loss: 682.527797\n",
      "Epoch 35, loss: 682.226770\n",
      "Epoch 36, loss: 681.815656\n",
      "Epoch 37, loss: 681.493264\n",
      "Epoch 38, loss: 681.587072\n",
      "Epoch 39, loss: 680.606010\n",
      "Epoch 40, loss: 682.362966\n",
      "Epoch 41, loss: 680.390931\n",
      "Epoch 42, loss: 680.577421\n",
      "Epoch 43, loss: 680.275618\n",
      "Epoch 44, loss: 680.145186\n",
      "Epoch 45, loss: 680.785050\n",
      "Epoch 46, loss: 680.654760\n",
      "Epoch 47, loss: 679.886897\n",
      "Epoch 48, loss: 679.903194\n",
      "Epoch 49, loss: 680.405623\n",
      "Epoch 50, loss: 679.403964\n",
      "Epoch 51, loss: 679.530567\n",
      "Epoch 52, loss: 679.904502\n",
      "Epoch 53, loss: 679.330342\n",
      "Epoch 54, loss: 679.115291\n",
      "Epoch 55, loss: 679.191268\n",
      "Epoch 56, loss: 679.316110\n",
      "Epoch 57, loss: 678.967463\n",
      "Epoch 58, loss: 678.929965\n",
      "Epoch 59, loss: 678.508610\n",
      "Epoch 60, loss: 678.844475\n",
      "Epoch 61, loss: 678.607970\n",
      "Epoch 62, loss: 677.463030\n",
      "Epoch 63, loss: 678.507666\n",
      "Epoch 64, loss: 678.342632\n",
      "Epoch 65, loss: 677.538187\n",
      "Epoch 66, loss: 677.954244\n",
      "Epoch 67, loss: 677.552938\n",
      "Epoch 68, loss: 677.829314\n",
      "Epoch 69, loss: 677.572685\n",
      "Epoch 70, loss: 677.661828\n",
      "Epoch 71, loss: 677.282814\n",
      "Epoch 72, loss: 677.355486\n",
      "Epoch 73, loss: 676.826799\n",
      "Epoch 74, loss: 676.741088\n",
      "Epoch 75, loss: 677.545004\n",
      "Epoch 76, loss: 676.654444\n",
      "Epoch 77, loss: 677.253301\n",
      "Epoch 78, loss: 676.962637\n",
      "Epoch 79, loss: 677.611949\n",
      "Epoch 80, loss: 676.950861\n",
      "Epoch 81, loss: 675.840548\n",
      "Epoch 82, loss: 676.507587\n",
      "Epoch 83, loss: 677.248837\n",
      "Epoch 84, loss: 676.800908\n",
      "Epoch 85, loss: 676.304135\n",
      "Epoch 86, loss: 676.022110\n",
      "Epoch 87, loss: 676.371313\n",
      "Epoch 88, loss: 676.508779\n",
      "Epoch 89, loss: 676.283745\n",
      "Epoch 90, loss: 675.595605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91, loss: 675.421766\n",
      "Epoch 92, loss: 675.287935\n",
      "Epoch 93, loss: 676.299523\n",
      "Epoch 94, loss: 675.699554\n",
      "Epoch 95, loss: 675.626147\n",
      "Epoch 96, loss: 676.140836\n",
      "Epoch 97, loss: 675.891339\n",
      "Epoch 98, loss: 675.161584\n",
      "Epoch 99, loss: 675.279091\n",
      "Epoch 100, loss: 675.343301\n",
      "Epoch 101, loss: 675.350036\n",
      "Epoch 102, loss: 675.501690\n",
      "Epoch 103, loss: 675.503107\n",
      "Epoch 104, loss: 676.132642\n",
      "Epoch 105, loss: 675.187423\n",
      "Epoch 106, loss: 675.471879\n",
      "Epoch 107, loss: 675.509322\n",
      "Epoch 108, loss: 674.674406\n",
      "Epoch 109, loss: 675.073277\n",
      "Epoch 110, loss: 675.430784\n",
      "Epoch 111, loss: 675.065581\n",
      "Epoch 112, loss: 675.104132\n",
      "Epoch 113, loss: 675.602954\n",
      "Epoch 114, loss: 674.536256\n",
      "Epoch 115, loss: 675.243830\n",
      "Epoch 116, loss: 675.417446\n",
      "Epoch 117, loss: 675.835909\n",
      "Epoch 118, loss: 674.567912\n",
      "Epoch 119, loss: 674.302235\n",
      "Epoch 120, loss: 673.675745\n",
      "Epoch 121, loss: 675.068184\n",
      "Epoch 122, loss: 674.869660\n",
      "Epoch 123, loss: 674.821136\n",
      "Epoch 124, loss: 674.982369\n",
      "Epoch 125, loss: 674.831759\n",
      "Epoch 126, loss: 674.811680\n",
      "Epoch 127, loss: 674.846371\n",
      "Epoch 128, loss: 675.301966\n",
      "Epoch 129, loss: 674.738006\n",
      "Epoch 130, loss: 674.161162\n",
      "Epoch 131, loss: 674.261144\n",
      "Epoch 132, loss: 674.159759\n",
      "Epoch 133, loss: 673.758183\n",
      "Epoch 134, loss: 674.309634\n",
      "Epoch 135, loss: 673.867564\n",
      "Epoch 136, loss: 673.910623\n",
      "Epoch 137, loss: 674.784112\n",
      "Epoch 138, loss: 674.163609\n",
      "Epoch 139, loss: 674.556273\n",
      "Epoch 140, loss: 674.431365\n",
      "Epoch 141, loss: 674.335653\n",
      "Epoch 142, loss: 673.194489\n",
      "Epoch 143, loss: 674.137727\n",
      "Epoch 144, loss: 674.536795\n",
      "Epoch 145, loss: 674.034461\n",
      "Epoch 146, loss: 673.456003\n",
      "Epoch 147, loss: 673.719446\n",
      "Epoch 148, loss: 673.654434\n",
      "Epoch 149, loss: 674.097002\n",
      "Epoch 150, loss: 673.659673\n",
      "Epoch 151, loss: 674.210813\n",
      "Epoch 152, loss: 674.496634\n",
      "Epoch 153, loss: 673.963590\n",
      "Epoch 154, loss: 673.083736\n",
      "Epoch 155, loss: 673.976031\n",
      "Epoch 156, loss: 673.947611\n",
      "Epoch 157, loss: 674.948904\n",
      "Epoch 158, loss: 673.719861\n",
      "Epoch 159, loss: 674.262795\n",
      "Epoch 160, loss: 673.758523\n",
      "Epoch 161, loss: 673.753381\n",
      "Epoch 162, loss: 674.571570\n",
      "Epoch 163, loss: 673.808260\n",
      "Epoch 164, loss: 673.498275\n",
      "Epoch 165, loss: 674.421787\n",
      "Epoch 166, loss: 673.292390\n",
      "Epoch 167, loss: 672.911866\n",
      "Epoch 168, loss: 673.822831\n",
      "Epoch 169, loss: 673.163965\n",
      "Epoch 170, loss: 673.778802\n",
      "Epoch 171, loss: 673.593910\n",
      "Epoch 172, loss: 673.397486\n",
      "Epoch 173, loss: 673.375482\n",
      "Epoch 174, loss: 673.271938\n",
      "Epoch 175, loss: 674.158361\n",
      "Epoch 176, loss: 673.537668\n",
      "Epoch 177, loss: 673.429248\n",
      "Epoch 178, loss: 673.717970\n",
      "Epoch 179, loss: 672.944256\n",
      "Epoch 180, loss: 673.402900\n",
      "Epoch 181, loss: 673.676229\n",
      "Epoch 182, loss: 673.481893\n",
      "Epoch 183, loss: 672.935602\n",
      "Epoch 184, loss: 673.270981\n",
      "Epoch 185, loss: 672.386165\n",
      "Epoch 186, loss: 673.627744\n",
      "Epoch 187, loss: 673.648027\n",
      "Epoch 188, loss: 673.147937\n",
      "Epoch 189, loss: 673.300029\n",
      "Epoch 190, loss: 673.390633\n",
      "Epoch 191, loss: 673.594635\n",
      "Epoch 192, loss: 673.703809\n",
      "Epoch 193, loss: 674.274952\n",
      "Epoch 194, loss: 674.627782\n",
      "Epoch 195, loss: 672.834128\n",
      "Epoch 196, loss: 673.009828\n",
      "Epoch 197, loss: 673.602398\n",
      "Epoch 198, loss: 673.428955\n",
      "Epoch 199, loss: 672.238189\n",
      "Epoch 0, loss: 690.678182\n",
      "Epoch 1, loss: 690.666933\n",
      "Epoch 2, loss: 690.150210\n",
      "Epoch 3, loss: 689.380688\n",
      "Epoch 4, loss: 689.137872\n",
      "Epoch 5, loss: 689.397151\n",
      "Epoch 6, loss: 688.648384\n",
      "Epoch 7, loss: 687.459668\n",
      "Epoch 8, loss: 688.024698\n",
      "Epoch 9, loss: 687.802974\n",
      "Epoch 10, loss: 687.333060\n",
      "Epoch 11, loss: 686.944968\n",
      "Epoch 12, loss: 686.616704\n",
      "Epoch 13, loss: 687.727822\n",
      "Epoch 14, loss: 686.116523\n",
      "Epoch 15, loss: 686.178488\n",
      "Epoch 16, loss: 685.935760\n",
      "Epoch 17, loss: 686.185392\n",
      "Epoch 18, loss: 684.923453\n",
      "Epoch 19, loss: 685.463449\n",
      "Epoch 20, loss: 684.882795\n",
      "Epoch 21, loss: 684.527927\n",
      "Epoch 22, loss: 684.309273\n",
      "Epoch 23, loss: 684.119703\n",
      "Epoch 24, loss: 683.639860\n",
      "Epoch 25, loss: 684.222151\n",
      "Epoch 26, loss: 683.612637\n",
      "Epoch 27, loss: 683.192904\n",
      "Epoch 28, loss: 683.267614\n",
      "Epoch 29, loss: 682.931146\n",
      "Epoch 30, loss: 682.768908\n",
      "Epoch 31, loss: 682.712519\n",
      "Epoch 32, loss: 683.105940\n",
      "Epoch 33, loss: 681.744682\n",
      "Epoch 34, loss: 682.052669\n",
      "Epoch 35, loss: 682.095580\n",
      "Epoch 36, loss: 681.572162\n",
      "Epoch 37, loss: 681.888363\n",
      "Epoch 38, loss: 681.636954\n",
      "Epoch 39, loss: 681.649257\n",
      "Epoch 40, loss: 680.925817\n",
      "Epoch 41, loss: 681.014644\n",
      "Epoch 42, loss: 680.870937\n",
      "Epoch 43, loss: 680.296723\n",
      "Epoch 44, loss: 680.796378\n",
      "Epoch 45, loss: 680.380719\n",
      "Epoch 46, loss: 680.428408\n",
      "Epoch 47, loss: 680.457717\n",
      "Epoch 48, loss: 679.715540\n",
      "Epoch 49, loss: 680.132475\n",
      "Epoch 50, loss: 680.724103\n",
      "Epoch 51, loss: 679.841851\n",
      "Epoch 52, loss: 679.985521\n",
      "Epoch 53, loss: 679.731640\n",
      "Epoch 54, loss: 678.819777\n",
      "Epoch 55, loss: 679.058267\n",
      "Epoch 56, loss: 679.108850\n",
      "Epoch 57, loss: 678.922398\n",
      "Epoch 58, loss: 679.036548\n",
      "Epoch 59, loss: 678.726722\n",
      "Epoch 60, loss: 678.782909\n",
      "Epoch 61, loss: 677.763414\n",
      "Epoch 62, loss: 678.430926\n",
      "Epoch 63, loss: 678.031078\n",
      "Epoch 64, loss: 677.840529\n",
      "Epoch 65, loss: 678.020347\n",
      "Epoch 66, loss: 677.324949\n",
      "Epoch 67, loss: 678.330980\n",
      "Epoch 68, loss: 676.775268\n",
      "Epoch 69, loss: 677.543869\n",
      "Epoch 70, loss: 676.901494\n",
      "Epoch 71, loss: 677.325181\n",
      "Epoch 72, loss: 677.351335\n",
      "Epoch 73, loss: 677.844769\n",
      "Epoch 74, loss: 676.677965\n",
      "Epoch 75, loss: 677.047320\n",
      "Epoch 76, loss: 677.164890\n",
      "Epoch 77, loss: 677.117742\n",
      "Epoch 78, loss: 677.193409\n",
      "Epoch 79, loss: 677.046456\n",
      "Epoch 80, loss: 676.633645\n",
      "Epoch 81, loss: 676.836672\n",
      "Epoch 82, loss: 677.009453\n",
      "Epoch 83, loss: 676.968649\n",
      "Epoch 84, loss: 676.565284\n",
      "Epoch 85, loss: 675.823151\n",
      "Epoch 86, loss: 677.299386\n",
      "Epoch 87, loss: 676.915340\n",
      "Epoch 88, loss: 677.148983\n",
      "Epoch 89, loss: 676.555434\n",
      "Epoch 90, loss: 675.778453\n",
      "Epoch 91, loss: 675.892087\n",
      "Epoch 92, loss: 676.111220\n",
      "Epoch 93, loss: 676.633317\n",
      "Epoch 94, loss: 675.568182\n",
      "Epoch 95, loss: 675.853569\n",
      "Epoch 96, loss: 675.425989\n",
      "Epoch 97, loss: 675.110682\n",
      "Epoch 98, loss: 675.837193\n",
      "Epoch 99, loss: 675.265275\n",
      "Epoch 100, loss: 676.606476\n",
      "Epoch 101, loss: 675.456537\n",
      "Epoch 102, loss: 675.255577\n",
      "Epoch 103, loss: 675.248313\n",
      "Epoch 104, loss: 675.899325\n",
      "Epoch 105, loss: 675.274551\n",
      "Epoch 106, loss: 675.292068\n",
      "Epoch 107, loss: 675.067728\n",
      "Epoch 108, loss: 674.908764\n",
      "Epoch 109, loss: 674.472428\n",
      "Epoch 110, loss: 676.000489\n",
      "Epoch 111, loss: 675.469375\n",
      "Epoch 112, loss: 675.523643\n",
      "Epoch 113, loss: 675.022613\n",
      "Epoch 114, loss: 674.529639\n",
      "Epoch 115, loss: 674.996699\n",
      "Epoch 116, loss: 675.424806\n",
      "Epoch 117, loss: 674.676888\n",
      "Epoch 118, loss: 675.330870\n",
      "Epoch 119, loss: 674.707008\n",
      "Epoch 120, loss: 674.087946\n",
      "Epoch 121, loss: 674.352965\n",
      "Epoch 122, loss: 674.996568\n",
      "Epoch 123, loss: 674.194808\n",
      "Epoch 124, loss: 674.936146\n",
      "Epoch 125, loss: 674.683139\n",
      "Epoch 126, loss: 674.781460\n",
      "Epoch 127, loss: 674.121190\n",
      "Epoch 128, loss: 674.472088\n",
      "Epoch 129, loss: 674.460973\n",
      "Epoch 130, loss: 674.676486\n",
      "Epoch 131, loss: 674.409471\n",
      "Epoch 132, loss: 674.293599\n",
      "Epoch 133, loss: 674.327120\n",
      "Epoch 134, loss: 674.187992\n",
      "Epoch 135, loss: 674.526803\n",
      "Epoch 136, loss: 673.759737\n",
      "Epoch 137, loss: 674.304455\n",
      "Epoch 138, loss: 673.743548\n",
      "Epoch 139, loss: 673.938223\n",
      "Epoch 140, loss: 674.100662\n",
      "Epoch 141, loss: 674.404327\n",
      "Epoch 142, loss: 673.969307\n",
      "Epoch 143, loss: 674.207874\n",
      "Epoch 144, loss: 673.924083\n",
      "Epoch 145, loss: 674.585321\n",
      "Epoch 146, loss: 674.123543\n",
      "Epoch 147, loss: 674.099633\n",
      "Epoch 148, loss: 674.110612\n",
      "Epoch 149, loss: 673.791152\n",
      "Epoch 150, loss: 674.053836\n",
      "Epoch 151, loss: 673.175931\n",
      "Epoch 152, loss: 673.569203\n",
      "Epoch 153, loss: 673.480654\n",
      "Epoch 154, loss: 673.693507\n",
      "Epoch 155, loss: 674.157926\n",
      "Epoch 156, loss: 674.755857\n",
      "Epoch 157, loss: 673.728373\n",
      "Epoch 158, loss: 674.183139\n",
      "Epoch 159, loss: 673.510190\n",
      "Epoch 160, loss: 674.004157\n",
      "Epoch 161, loss: 673.954423\n",
      "Epoch 162, loss: 673.030580\n",
      "Epoch 163, loss: 674.187772\n",
      "Epoch 164, loss: 674.270788\n",
      "Epoch 165, loss: 673.894192\n",
      "Epoch 166, loss: 674.083301\n",
      "Epoch 167, loss: 674.163433\n",
      "Epoch 168, loss: 673.696442\n",
      "Epoch 169, loss: 673.389804\n",
      "Epoch 170, loss: 673.556739\n",
      "Epoch 171, loss: 673.360835\n",
      "Epoch 172, loss: 673.852738\n",
      "Epoch 173, loss: 673.835976\n",
      "Epoch 174, loss: 673.490257\n",
      "Epoch 175, loss: 673.384930\n",
      "Epoch 176, loss: 673.624441\n",
      "Epoch 177, loss: 673.296312\n",
      "Epoch 178, loss: 673.338097\n",
      "Epoch 179, loss: 674.043970\n",
      "Epoch 180, loss: 673.363268\n",
      "Epoch 181, loss: 672.982101\n",
      "Epoch 182, loss: 673.329700\n",
      "Epoch 183, loss: 673.757622\n",
      "Epoch 184, loss: 673.966891\n",
      "Epoch 185, loss: 673.561259\n",
      "Epoch 186, loss: 673.045604\n",
      "Epoch 187, loss: 673.194147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188, loss: 674.296845\n",
      "Epoch 189, loss: 672.460345\n",
      "Epoch 190, loss: 673.682891\n",
      "Epoch 191, loss: 673.983340\n",
      "Epoch 192, loss: 673.679935\n",
      "Epoch 193, loss: 673.385000\n",
      "Epoch 194, loss: 672.716758\n",
      "Epoch 195, loss: 672.859053\n",
      "Epoch 196, loss: 673.548833\n",
      "Epoch 197, loss: 672.892149\n",
      "Epoch 198, loss: 673.474875\n",
      "Epoch 199, loss: 674.202413\n",
      "Epoch 0, loss: 690.562183\n",
      "Epoch 1, loss: 690.867336\n",
      "Epoch 2, loss: 690.961444\n",
      "Epoch 3, loss: 690.700064\n",
      "Epoch 4, loss: 690.629748\n",
      "Epoch 5, loss: 690.641490\n",
      "Epoch 6, loss: 690.332787\n",
      "Epoch 7, loss: 690.348032\n",
      "Epoch 8, loss: 690.541502\n",
      "Epoch 9, loss: 690.370002\n",
      "Epoch 10, loss: 690.643058\n",
      "Epoch 11, loss: 690.525330\n",
      "Epoch 12, loss: 690.226112\n",
      "Epoch 13, loss: 690.287151\n",
      "Epoch 14, loss: 690.213435\n",
      "Epoch 15, loss: 690.555121\n",
      "Epoch 16, loss: 690.378945\n",
      "Epoch 17, loss: 690.235605\n",
      "Epoch 18, loss: 690.190355\n",
      "Epoch 19, loss: 690.062318\n",
      "Epoch 20, loss: 690.054474\n",
      "Epoch 21, loss: 690.083653\n",
      "Epoch 22, loss: 690.335365\n",
      "Epoch 23, loss: 690.122632\n",
      "Epoch 24, loss: 690.005060\n",
      "Epoch 25, loss: 690.169225\n",
      "Epoch 26, loss: 689.766137\n",
      "Epoch 27, loss: 689.580191\n",
      "Epoch 28, loss: 689.801080\n",
      "Epoch 29, loss: 689.657482\n",
      "Epoch 30, loss: 689.617140\n",
      "Epoch 31, loss: 689.565630\n",
      "Epoch 32, loss: 690.032295\n",
      "Epoch 33, loss: 689.697503\n",
      "Epoch 34, loss: 689.756872\n",
      "Epoch 35, loss: 689.462821\n",
      "Epoch 36, loss: 689.787093\n",
      "Epoch 37, loss: 689.820276\n",
      "Epoch 38, loss: 689.489747\n",
      "Epoch 39, loss: 689.595979\n",
      "Epoch 40, loss: 689.497802\n",
      "Epoch 41, loss: 689.538092\n",
      "Epoch 42, loss: 689.446495\n",
      "Epoch 43, loss: 689.334738\n",
      "Epoch 44, loss: 689.326529\n",
      "Epoch 45, loss: 689.280572\n",
      "Epoch 46, loss: 689.216922\n",
      "Epoch 47, loss: 689.452297\n",
      "Epoch 48, loss: 689.529896\n",
      "Epoch 49, loss: 689.128674\n",
      "Epoch 50, loss: 689.336806\n",
      "Epoch 51, loss: 688.984866\n",
      "Epoch 52, loss: 689.164574\n",
      "Epoch 53, loss: 689.030910\n",
      "Epoch 54, loss: 689.015991\n",
      "Epoch 55, loss: 689.253809\n",
      "Epoch 56, loss: 689.316822\n",
      "Epoch 57, loss: 688.886973\n",
      "Epoch 58, loss: 688.967336\n",
      "Epoch 59, loss: 688.796502\n",
      "Epoch 60, loss: 688.862268\n",
      "Epoch 61, loss: 688.976873\n",
      "Epoch 62, loss: 688.937804\n",
      "Epoch 63, loss: 688.637404\n",
      "Epoch 64, loss: 688.624663\n",
      "Epoch 65, loss: 688.564612\n",
      "Epoch 66, loss: 689.034152\n",
      "Epoch 67, loss: 688.690106\n",
      "Epoch 68, loss: 688.662191\n",
      "Epoch 69, loss: 688.528236\n",
      "Epoch 70, loss: 688.384843\n",
      "Epoch 71, loss: 688.680702\n",
      "Epoch 72, loss: 688.554288\n",
      "Epoch 73, loss: 688.635770\n",
      "Epoch 74, loss: 688.361191\n",
      "Epoch 75, loss: 688.349707\n",
      "Epoch 76, loss: 688.175245\n",
      "Epoch 77, loss: 688.597472\n",
      "Epoch 78, loss: 688.210835\n",
      "Epoch 79, loss: 688.346918\n",
      "Epoch 80, loss: 688.119070\n",
      "Epoch 81, loss: 688.219003\n",
      "Epoch 82, loss: 688.098389\n",
      "Epoch 83, loss: 688.101160\n",
      "Epoch 84, loss: 688.115998\n",
      "Epoch 85, loss: 688.085105\n",
      "Epoch 86, loss: 688.083315\n",
      "Epoch 87, loss: 688.089380\n",
      "Epoch 88, loss: 687.950039\n",
      "Epoch 89, loss: 687.879233\n",
      "Epoch 90, loss: 688.399438\n",
      "Epoch 91, loss: 688.165292\n",
      "Epoch 92, loss: 688.081208\n",
      "Epoch 93, loss: 687.749006\n",
      "Epoch 94, loss: 687.786862\n",
      "Epoch 95, loss: 687.868299\n",
      "Epoch 96, loss: 687.585490\n",
      "Epoch 97, loss: 687.738773\n",
      "Epoch 98, loss: 687.789069\n",
      "Epoch 99, loss: 687.542255\n",
      "Epoch 100, loss: 687.656831\n",
      "Epoch 101, loss: 687.774313\n",
      "Epoch 102, loss: 687.558677\n",
      "Epoch 103, loss: 687.456458\n",
      "Epoch 104, loss: 687.514557\n",
      "Epoch 105, loss: 687.561348\n",
      "Epoch 106, loss: 687.314660\n",
      "Epoch 107, loss: 687.479367\n",
      "Epoch 108, loss: 687.574395\n",
      "Epoch 109, loss: 687.325082\n",
      "Epoch 110, loss: 687.518521\n",
      "Epoch 111, loss: 687.449224\n",
      "Epoch 112, loss: 687.467550\n",
      "Epoch 113, loss: 687.161346\n",
      "Epoch 114, loss: 687.408439\n",
      "Epoch 115, loss: 687.354670\n",
      "Epoch 116, loss: 687.348415\n",
      "Epoch 117, loss: 687.380791\n",
      "Epoch 118, loss: 687.196839\n",
      "Epoch 119, loss: 686.921218\n",
      "Epoch 120, loss: 686.883818\n",
      "Epoch 121, loss: 687.321433\n",
      "Epoch 122, loss: 687.104544\n",
      "Epoch 123, loss: 686.957051\n",
      "Epoch 124, loss: 686.991396\n",
      "Epoch 125, loss: 686.803780\n",
      "Epoch 126, loss: 687.003137\n",
      "Epoch 127, loss: 686.910093\n",
      "Epoch 128, loss: 686.938540\n",
      "Epoch 129, loss: 686.787993\n",
      "Epoch 130, loss: 686.896756\n",
      "Epoch 131, loss: 686.833871\n",
      "Epoch 132, loss: 686.765179\n",
      "Epoch 133, loss: 687.168249\n",
      "Epoch 134, loss: 686.813135\n",
      "Epoch 135, loss: 686.827608\n",
      "Epoch 136, loss: 686.641414\n",
      "Epoch 137, loss: 686.930761\n",
      "Epoch 138, loss: 686.426671\n",
      "Epoch 139, loss: 686.505506\n",
      "Epoch 140, loss: 686.503860\n",
      "Epoch 141, loss: 686.601895\n",
      "Epoch 142, loss: 686.581327\n",
      "Epoch 143, loss: 686.433750\n",
      "Epoch 144, loss: 686.220480\n",
      "Epoch 145, loss: 686.237835\n",
      "Epoch 146, loss: 686.365325\n",
      "Epoch 147, loss: 686.654135\n",
      "Epoch 148, loss: 686.395452\n",
      "Epoch 149, loss: 686.515847\n",
      "Epoch 150, loss: 686.359528\n",
      "Epoch 151, loss: 686.148663\n",
      "Epoch 152, loss: 686.374746\n",
      "Epoch 153, loss: 686.393087\n",
      "Epoch 154, loss: 686.378257\n",
      "Epoch 155, loss: 686.401690\n",
      "Epoch 156, loss: 686.414319\n",
      "Epoch 157, loss: 686.353879\n",
      "Epoch 158, loss: 686.088854\n",
      "Epoch 159, loss: 686.487853\n",
      "Epoch 160, loss: 686.298350\n",
      "Epoch 161, loss: 686.185374\n",
      "Epoch 162, loss: 686.142147\n",
      "Epoch 163, loss: 686.180145\n",
      "Epoch 164, loss: 686.103714\n",
      "Epoch 165, loss: 686.125798\n",
      "Epoch 166, loss: 686.015482\n",
      "Epoch 167, loss: 685.649338\n",
      "Epoch 168, loss: 685.793063\n",
      "Epoch 169, loss: 686.015408\n",
      "Epoch 170, loss: 685.670811\n",
      "Epoch 171, loss: 685.735823\n",
      "Epoch 172, loss: 685.389979\n",
      "Epoch 173, loss: 685.841650\n",
      "Epoch 174, loss: 685.877473\n",
      "Epoch 175, loss: 685.836754\n",
      "Epoch 176, loss: 685.665511\n",
      "Epoch 177, loss: 685.815358\n",
      "Epoch 178, loss: 685.597114\n",
      "Epoch 179, loss: 685.807816\n",
      "Epoch 180, loss: 685.617259\n",
      "Epoch 181, loss: 685.762520\n",
      "Epoch 182, loss: 685.478615\n",
      "Epoch 183, loss: 685.573260\n",
      "Epoch 184, loss: 685.524254\n",
      "Epoch 185, loss: 685.413743\n",
      "Epoch 186, loss: 685.469695\n",
      "Epoch 187, loss: 685.295643\n",
      "Epoch 188, loss: 685.513041\n",
      "Epoch 189, loss: 685.360135\n",
      "Epoch 190, loss: 685.260455\n",
      "Epoch 191, loss: 685.185711\n",
      "Epoch 192, loss: 685.413860\n",
      "Epoch 193, loss: 685.555107\n",
      "Epoch 194, loss: 685.379074\n",
      "Epoch 195, loss: 685.342747\n",
      "Epoch 196, loss: 685.358572\n",
      "Epoch 197, loss: 685.223248\n",
      "Epoch 198, loss: 685.378803\n",
      "Epoch 199, loss: 685.034800\n",
      "Epoch 0, loss: 690.659259\n",
      "Epoch 1, loss: 690.686909\n",
      "Epoch 2, loss: 690.745679\n",
      "Epoch 3, loss: 690.583740\n",
      "Epoch 4, loss: 690.451682\n",
      "Epoch 5, loss: 690.368519\n",
      "Epoch 6, loss: 690.894403\n",
      "Epoch 7, loss: 690.601009\n",
      "Epoch 8, loss: 690.439302\n",
      "Epoch 9, loss: 690.720640\n",
      "Epoch 10, loss: 690.568024\n",
      "Epoch 11, loss: 690.295341\n",
      "Epoch 12, loss: 690.376632\n",
      "Epoch 13, loss: 690.223215\n",
      "Epoch 14, loss: 689.991030\n",
      "Epoch 15, loss: 690.206143\n",
      "Epoch 16, loss: 690.149303\n",
      "Epoch 17, loss: 690.054403\n",
      "Epoch 18, loss: 690.345345\n",
      "Epoch 19, loss: 690.057953\n",
      "Epoch 20, loss: 690.343193\n",
      "Epoch 21, loss: 690.200651\n",
      "Epoch 22, loss: 690.021199\n",
      "Epoch 23, loss: 690.150207\n",
      "Epoch 24, loss: 689.852062\n",
      "Epoch 25, loss: 690.120216\n",
      "Epoch 26, loss: 690.160691\n",
      "Epoch 27, loss: 689.658529\n",
      "Epoch 28, loss: 690.042540\n",
      "Epoch 29, loss: 689.417530\n",
      "Epoch 30, loss: 689.946186\n",
      "Epoch 31, loss: 689.571731\n",
      "Epoch 32, loss: 689.843176\n",
      "Epoch 33, loss: 689.861480\n",
      "Epoch 34, loss: 689.864403\n",
      "Epoch 35, loss: 689.612773\n",
      "Epoch 36, loss: 689.715566\n",
      "Epoch 37, loss: 689.592648\n",
      "Epoch 38, loss: 689.436515\n",
      "Epoch 39, loss: 689.844184\n",
      "Epoch 40, loss: 689.407512\n",
      "Epoch 41, loss: 689.317424\n",
      "Epoch 42, loss: 689.279424\n",
      "Epoch 43, loss: 689.382463\n",
      "Epoch 44, loss: 689.370571\n",
      "Epoch 45, loss: 689.581766\n",
      "Epoch 46, loss: 689.182543\n",
      "Epoch 47, loss: 689.142846\n",
      "Epoch 48, loss: 689.234141\n",
      "Epoch 49, loss: 689.500214\n",
      "Epoch 50, loss: 689.299168\n",
      "Epoch 51, loss: 689.217906\n",
      "Epoch 52, loss: 689.001235\n",
      "Epoch 53, loss: 689.197369\n",
      "Epoch 54, loss: 689.099288\n",
      "Epoch 55, loss: 689.277354\n",
      "Epoch 56, loss: 688.801965\n",
      "Epoch 57, loss: 688.997135\n",
      "Epoch 58, loss: 689.045333\n",
      "Epoch 59, loss: 688.956445\n",
      "Epoch 60, loss: 689.158065\n",
      "Epoch 61, loss: 688.971337\n",
      "Epoch 62, loss: 688.610208\n",
      "Epoch 63, loss: 688.858531\n",
      "Epoch 64, loss: 688.868876\n",
      "Epoch 65, loss: 688.449377\n",
      "Epoch 66, loss: 688.572536\n",
      "Epoch 67, loss: 688.791184\n",
      "Epoch 68, loss: 688.793304\n",
      "Epoch 69, loss: 688.598909\n",
      "Epoch 70, loss: 688.536309\n",
      "Epoch 71, loss: 688.574565\n",
      "Epoch 72, loss: 688.326276\n",
      "Epoch 73, loss: 688.439113\n",
      "Epoch 74, loss: 688.338116\n",
      "Epoch 75, loss: 688.204205\n",
      "Epoch 76, loss: 688.458045\n",
      "Epoch 77, loss: 688.584630\n",
      "Epoch 78, loss: 688.001050\n",
      "Epoch 79, loss: 688.332176\n",
      "Epoch 80, loss: 688.637899\n",
      "Epoch 81, loss: 688.352869\n",
      "Epoch 82, loss: 688.290356\n",
      "Epoch 83, loss: 688.370501\n",
      "Epoch 84, loss: 688.196682\n",
      "Epoch 85, loss: 687.966157\n",
      "Epoch 86, loss: 687.825427\n",
      "Epoch 87, loss: 688.036102\n",
      "Epoch 88, loss: 688.004666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89, loss: 687.791646\n",
      "Epoch 90, loss: 687.981863\n",
      "Epoch 91, loss: 688.115831\n",
      "Epoch 92, loss: 687.946748\n",
      "Epoch 93, loss: 688.073289\n",
      "Epoch 94, loss: 687.827820\n",
      "Epoch 95, loss: 687.778228\n",
      "Epoch 96, loss: 687.880254\n",
      "Epoch 97, loss: 687.794532\n",
      "Epoch 98, loss: 687.759846\n",
      "Epoch 99, loss: 688.026240\n",
      "Epoch 100, loss: 687.679728\n",
      "Epoch 101, loss: 687.640107\n",
      "Epoch 102, loss: 687.628487\n",
      "Epoch 103, loss: 687.176478\n",
      "Epoch 104, loss: 687.566045\n",
      "Epoch 105, loss: 687.750398\n",
      "Epoch 106, loss: 687.503826\n",
      "Epoch 107, loss: 687.531753\n",
      "Epoch 108, loss: 687.669370\n",
      "Epoch 109, loss: 687.391928\n",
      "Epoch 110, loss: 687.266888\n",
      "Epoch 111, loss: 687.541021\n",
      "Epoch 112, loss: 687.329761\n",
      "Epoch 113, loss: 687.362444\n",
      "Epoch 114, loss: 687.108035\n",
      "Epoch 115, loss: 687.200553\n",
      "Epoch 116, loss: 687.147891\n",
      "Epoch 117, loss: 687.101211\n",
      "Epoch 118, loss: 686.903816\n",
      "Epoch 119, loss: 687.157196\n",
      "Epoch 120, loss: 687.056708\n",
      "Epoch 121, loss: 687.409905\n",
      "Epoch 122, loss: 687.209514\n",
      "Epoch 123, loss: 687.000156\n",
      "Epoch 124, loss: 687.127660\n",
      "Epoch 125, loss: 687.079137\n",
      "Epoch 126, loss: 687.007147\n",
      "Epoch 127, loss: 686.907269\n",
      "Epoch 128, loss: 687.270861\n",
      "Epoch 129, loss: 686.571648\n",
      "Epoch 130, loss: 687.113167\n",
      "Epoch 131, loss: 686.778938\n",
      "Epoch 132, loss: 687.025178\n",
      "Epoch 133, loss: 686.828577\n",
      "Epoch 134, loss: 686.801228\n",
      "Epoch 135, loss: 686.803254\n",
      "Epoch 136, loss: 686.991058\n",
      "Epoch 137, loss: 686.752323\n",
      "Epoch 138, loss: 686.527378\n",
      "Epoch 139, loss: 686.692932\n",
      "Epoch 140, loss: 686.525861\n",
      "Epoch 141, loss: 686.633790\n",
      "Epoch 142, loss: 686.440604\n",
      "Epoch 143, loss: 686.049940\n",
      "Epoch 144, loss: 686.540294\n",
      "Epoch 145, loss: 686.506265\n",
      "Epoch 146, loss: 686.437467\n",
      "Epoch 147, loss: 686.297582\n",
      "Epoch 148, loss: 686.301215\n",
      "Epoch 149, loss: 686.238683\n",
      "Epoch 150, loss: 686.242236\n",
      "Epoch 151, loss: 686.304213\n",
      "Epoch 152, loss: 686.408653\n",
      "Epoch 153, loss: 686.086031\n",
      "Epoch 154, loss: 686.049060\n",
      "Epoch 155, loss: 686.136997\n",
      "Epoch 156, loss: 686.266514\n",
      "Epoch 157, loss: 685.646661\n",
      "Epoch 158, loss: 686.031350\n",
      "Epoch 159, loss: 686.012645\n",
      "Epoch 160, loss: 686.047726\n",
      "Epoch 161, loss: 686.173142\n",
      "Epoch 162, loss: 686.075805\n",
      "Epoch 163, loss: 686.093298\n",
      "Epoch 164, loss: 685.982700\n",
      "Epoch 165, loss: 685.959563\n",
      "Epoch 166, loss: 685.835759\n",
      "Epoch 167, loss: 685.748470\n",
      "Epoch 168, loss: 685.853777\n",
      "Epoch 169, loss: 685.909346\n",
      "Epoch 170, loss: 685.906710\n",
      "Epoch 171, loss: 686.036673\n",
      "Epoch 172, loss: 685.767768\n",
      "Epoch 173, loss: 685.689884\n",
      "Epoch 174, loss: 685.712037\n",
      "Epoch 175, loss: 685.622746\n",
      "Epoch 176, loss: 685.318325\n",
      "Epoch 177, loss: 685.644756\n",
      "Epoch 178, loss: 685.499862\n",
      "Epoch 179, loss: 685.620228\n",
      "Epoch 180, loss: 685.499188\n",
      "Epoch 181, loss: 685.406881\n",
      "Epoch 182, loss: 685.444285\n",
      "Epoch 183, loss: 685.475911\n",
      "Epoch 184, loss: 685.414025\n",
      "Epoch 185, loss: 685.432145\n",
      "Epoch 186, loss: 685.305520\n",
      "Epoch 187, loss: 685.677906\n",
      "Epoch 188, loss: 685.328854\n",
      "Epoch 189, loss: 685.365551\n",
      "Epoch 190, loss: 685.116984\n",
      "Epoch 191, loss: 685.349419\n",
      "Epoch 192, loss: 685.397488\n",
      "Epoch 193, loss: 685.308781\n",
      "Epoch 194, loss: 685.255145\n",
      "Epoch 195, loss: 685.034512\n",
      "Epoch 196, loss: 685.241608\n",
      "Epoch 197, loss: 685.260628\n",
      "Epoch 198, loss: 685.178049\n",
      "Epoch 199, loss: 685.592891\n",
      "Epoch 0, loss: 690.730740\n",
      "Epoch 1, loss: 690.730475\n",
      "Epoch 2, loss: 690.771849\n",
      "Epoch 3, loss: 690.626439\n",
      "Epoch 4, loss: 690.727198\n",
      "Epoch 5, loss: 690.913577\n",
      "Epoch 6, loss: 690.526481\n",
      "Epoch 7, loss: 690.527092\n",
      "Epoch 8, loss: 690.192381\n",
      "Epoch 9, loss: 690.464817\n",
      "Epoch 10, loss: 690.533289\n",
      "Epoch 11, loss: 690.394034\n",
      "Epoch 12, loss: 690.348440\n",
      "Epoch 13, loss: 690.476151\n",
      "Epoch 14, loss: 690.422861\n",
      "Epoch 15, loss: 689.947716\n",
      "Epoch 16, loss: 690.193498\n",
      "Epoch 17, loss: 690.171691\n",
      "Epoch 18, loss: 690.503547\n",
      "Epoch 19, loss: 690.163963\n",
      "Epoch 20, loss: 690.052935\n",
      "Epoch 21, loss: 689.957738\n",
      "Epoch 22, loss: 689.873328\n",
      "Epoch 23, loss: 689.899185\n",
      "Epoch 24, loss: 690.226467\n",
      "Epoch 25, loss: 690.157473\n",
      "Epoch 26, loss: 689.855595\n",
      "Epoch 27, loss: 689.726841\n",
      "Epoch 28, loss: 689.758478\n",
      "Epoch 29, loss: 689.699459\n",
      "Epoch 30, loss: 689.704934\n",
      "Epoch 31, loss: 689.794736\n",
      "Epoch 32, loss: 689.640095\n",
      "Epoch 33, loss: 689.786647\n",
      "Epoch 34, loss: 689.442848\n",
      "Epoch 35, loss: 689.522730\n",
      "Epoch 36, loss: 689.609871\n",
      "Epoch 37, loss: 689.593608\n",
      "Epoch 38, loss: 689.640722\n",
      "Epoch 39, loss: 689.518680\n",
      "Epoch 40, loss: 689.555602\n",
      "Epoch 41, loss: 689.486453\n",
      "Epoch 42, loss: 689.380284\n",
      "Epoch 43, loss: 689.411467\n",
      "Epoch 44, loss: 689.477212\n",
      "Epoch 45, loss: 689.576029\n",
      "Epoch 46, loss: 689.035840\n",
      "Epoch 47, loss: 689.568973\n",
      "Epoch 48, loss: 689.579540\n",
      "Epoch 49, loss: 689.220155\n",
      "Epoch 50, loss: 689.034473\n",
      "Epoch 51, loss: 688.830768\n",
      "Epoch 52, loss: 689.146590\n",
      "Epoch 53, loss: 689.042407\n",
      "Epoch 54, loss: 689.134358\n",
      "Epoch 55, loss: 689.038042\n",
      "Epoch 56, loss: 689.043707\n",
      "Epoch 57, loss: 688.717077\n",
      "Epoch 58, loss: 689.004470\n",
      "Epoch 59, loss: 689.167546\n",
      "Epoch 60, loss: 689.091116\n",
      "Epoch 61, loss: 688.799154\n",
      "Epoch 62, loss: 688.681530\n",
      "Epoch 63, loss: 688.964325\n",
      "Epoch 64, loss: 689.056786\n",
      "Epoch 65, loss: 688.747763\n",
      "Epoch 66, loss: 688.634562\n",
      "Epoch 67, loss: 688.543239\n",
      "Epoch 68, loss: 688.588792\n",
      "Epoch 69, loss: 688.733404\n",
      "Epoch 70, loss: 688.418200\n",
      "Epoch 71, loss: 688.499612\n",
      "Epoch 72, loss: 688.442393\n",
      "Epoch 73, loss: 688.426557\n",
      "Epoch 74, loss: 688.360453\n",
      "Epoch 75, loss: 688.141778\n",
      "Epoch 76, loss: 688.280503\n",
      "Epoch 77, loss: 688.491375\n",
      "Epoch 78, loss: 688.534549\n",
      "Epoch 79, loss: 688.402848\n",
      "Epoch 80, loss: 688.175117\n",
      "Epoch 81, loss: 688.377986\n",
      "Epoch 82, loss: 688.079691\n",
      "Epoch 83, loss: 688.299655\n",
      "Epoch 84, loss: 687.979311\n",
      "Epoch 85, loss: 688.186607\n",
      "Epoch 86, loss: 688.226076\n",
      "Epoch 87, loss: 688.359479\n",
      "Epoch 88, loss: 687.957692\n",
      "Epoch 89, loss: 687.805194\n",
      "Epoch 90, loss: 687.813510\n",
      "Epoch 91, loss: 688.105282\n",
      "Epoch 92, loss: 687.823958\n",
      "Epoch 93, loss: 687.726386\n",
      "Epoch 94, loss: 687.760412\n",
      "Epoch 95, loss: 687.740577\n",
      "Epoch 96, loss: 687.834787\n",
      "Epoch 97, loss: 687.860408\n",
      "Epoch 98, loss: 687.614517\n",
      "Epoch 99, loss: 687.711714\n",
      "Epoch 100, loss: 687.438980\n",
      "Epoch 101, loss: 687.739555\n",
      "Epoch 102, loss: 687.604382\n",
      "Epoch 103, loss: 687.502814\n",
      "Epoch 104, loss: 687.578199\n",
      "Epoch 105, loss: 687.772166\n",
      "Epoch 106, loss: 687.691712\n",
      "Epoch 107, loss: 687.434610\n",
      "Epoch 108, loss: 687.440938\n",
      "Epoch 109, loss: 687.365806\n",
      "Epoch 110, loss: 687.533437\n",
      "Epoch 111, loss: 687.355399\n",
      "Epoch 112, loss: 687.289360\n",
      "Epoch 113, loss: 687.395108\n",
      "Epoch 114, loss: 687.326142\n",
      "Epoch 115, loss: 687.259787\n",
      "Epoch 116, loss: 687.386872\n",
      "Epoch 117, loss: 687.174261\n",
      "Epoch 118, loss: 687.119374\n",
      "Epoch 119, loss: 687.202452\n",
      "Epoch 120, loss: 687.193285\n",
      "Epoch 121, loss: 687.272030\n",
      "Epoch 122, loss: 687.168451\n",
      "Epoch 123, loss: 687.049069\n",
      "Epoch 124, loss: 687.052690\n",
      "Epoch 125, loss: 686.696958\n",
      "Epoch 126, loss: 686.848414\n",
      "Epoch 127, loss: 686.987988\n",
      "Epoch 128, loss: 686.983436\n",
      "Epoch 129, loss: 686.664714\n",
      "Epoch 130, loss: 686.690349\n",
      "Epoch 131, loss: 687.125208\n",
      "Epoch 132, loss: 686.979847\n",
      "Epoch 133, loss: 686.684091\n",
      "Epoch 134, loss: 686.661944\n",
      "Epoch 135, loss: 686.675818\n",
      "Epoch 136, loss: 686.765559\n",
      "Epoch 137, loss: 686.477359\n",
      "Epoch 138, loss: 686.857806\n",
      "Epoch 139, loss: 686.656244\n",
      "Epoch 140, loss: 686.417112\n",
      "Epoch 141, loss: 686.389437\n",
      "Epoch 142, loss: 686.644016\n",
      "Epoch 143, loss: 686.353719\n",
      "Epoch 144, loss: 686.530640\n",
      "Epoch 145, loss: 686.331625\n",
      "Epoch 146, loss: 686.581206\n",
      "Epoch 147, loss: 686.329818\n",
      "Epoch 148, loss: 686.427140\n",
      "Epoch 149, loss: 686.132377\n",
      "Epoch 150, loss: 686.072166\n",
      "Epoch 151, loss: 686.098387\n",
      "Epoch 152, loss: 686.311124\n",
      "Epoch 153, loss: 685.765377\n",
      "Epoch 154, loss: 686.352708\n",
      "Epoch 155, loss: 686.405127\n",
      "Epoch 156, loss: 686.048174\n",
      "Epoch 157, loss: 686.083373\n",
      "Epoch 158, loss: 686.109110\n",
      "Epoch 159, loss: 685.926996\n",
      "Epoch 160, loss: 686.129452\n",
      "Epoch 161, loss: 686.252167\n",
      "Epoch 162, loss: 686.181050\n",
      "Epoch 163, loss: 685.956267\n",
      "Epoch 164, loss: 685.926836\n",
      "Epoch 165, loss: 685.974994\n",
      "Epoch 166, loss: 686.114224\n",
      "Epoch 167, loss: 685.856688\n",
      "Epoch 168, loss: 685.813972\n",
      "Epoch 169, loss: 685.679472\n",
      "Epoch 170, loss: 685.893826\n",
      "Epoch 171, loss: 685.998042\n",
      "Epoch 172, loss: 685.702214\n",
      "Epoch 173, loss: 685.502589\n",
      "Epoch 174, loss: 685.761157\n",
      "Epoch 175, loss: 685.845153\n",
      "Epoch 176, loss: 685.519641\n",
      "Epoch 177, loss: 685.613700\n",
      "Epoch 178, loss: 685.623094\n",
      "Epoch 179, loss: 685.506143\n",
      "Epoch 180, loss: 685.525287\n",
      "Epoch 181, loss: 685.493363\n",
      "Epoch 182, loss: 685.886667\n",
      "Epoch 183, loss: 685.721741\n",
      "Epoch 184, loss: 685.448396\n",
      "Epoch 185, loss: 685.187763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 186, loss: 685.174517\n",
      "Epoch 187, loss: 685.554282\n",
      "Epoch 188, loss: 685.473579\n",
      "Epoch 189, loss: 685.668541\n",
      "Epoch 190, loss: 685.468775\n",
      "Epoch 191, loss: 685.427287\n",
      "Epoch 192, loss: 685.285435\n",
      "Epoch 193, loss: 685.456942\n",
      "Epoch 194, loss: 685.448886\n",
      "Epoch 195, loss: 685.344979\n",
      "Epoch 196, loss: 685.289958\n",
      "Epoch 197, loss: 685.037578\n",
      "Epoch 198, loss: 684.968302\n",
      "Epoch 199, loss: 685.001083\n",
      "Epoch 0, loss: 690.925657\n",
      "Epoch 1, loss: 690.921694\n",
      "Epoch 2, loss: 691.119248\n",
      "Epoch 3, loss: 690.983626\n",
      "Epoch 4, loss: 690.416158\n",
      "Epoch 5, loss: 690.754309\n",
      "Epoch 6, loss: 690.672454\n",
      "Epoch 7, loss: 690.424888\n",
      "Epoch 8, loss: 690.223714\n",
      "Epoch 9, loss: 690.380569\n",
      "Epoch 10, loss: 690.303091\n",
      "Epoch 11, loss: 690.391843\n",
      "Epoch 12, loss: 690.178858\n",
      "Epoch 13, loss: 690.275523\n",
      "Epoch 14, loss: 690.294909\n",
      "Epoch 15, loss: 690.266398\n",
      "Epoch 16, loss: 690.144406\n",
      "Epoch 17, loss: 690.221980\n",
      "Epoch 18, loss: 690.171738\n",
      "Epoch 19, loss: 689.956243\n",
      "Epoch 20, loss: 690.087854\n",
      "Epoch 21, loss: 689.823868\n",
      "Epoch 22, loss: 690.012279\n",
      "Epoch 23, loss: 690.116811\n",
      "Epoch 24, loss: 690.129556\n",
      "Epoch 25, loss: 690.232538\n",
      "Epoch 26, loss: 689.836122\n",
      "Epoch 27, loss: 689.970557\n",
      "Epoch 28, loss: 689.510735\n",
      "Epoch 29, loss: 689.957005\n",
      "Epoch 30, loss: 689.931292\n",
      "Epoch 31, loss: 689.700763\n",
      "Epoch 32, loss: 689.797808\n",
      "Epoch 33, loss: 689.743402\n",
      "Epoch 34, loss: 689.668487\n",
      "Epoch 35, loss: 689.614639\n",
      "Epoch 36, loss: 689.549672\n",
      "Epoch 37, loss: 689.608308\n",
      "Epoch 38, loss: 689.426449\n",
      "Epoch 39, loss: 689.251263\n",
      "Epoch 40, loss: 689.371990\n",
      "Epoch 41, loss: 689.362469\n",
      "Epoch 42, loss: 689.649698\n",
      "Epoch 43, loss: 689.098426\n",
      "Epoch 44, loss: 689.330589\n",
      "Epoch 45, loss: 689.038121\n",
      "Epoch 46, loss: 689.246458\n",
      "Epoch 47, loss: 689.528894\n",
      "Epoch 48, loss: 689.612000\n",
      "Epoch 49, loss: 689.138520\n",
      "Epoch 50, loss: 689.086508\n",
      "Epoch 51, loss: 689.117636\n",
      "Epoch 52, loss: 689.071129\n",
      "Epoch 53, loss: 688.956655\n",
      "Epoch 54, loss: 689.155003\n",
      "Epoch 55, loss: 689.044107\n",
      "Epoch 56, loss: 688.894127\n",
      "Epoch 57, loss: 689.147464\n",
      "Epoch 58, loss: 689.091154\n",
      "Epoch 59, loss: 689.084339\n",
      "Epoch 60, loss: 688.876619\n",
      "Epoch 61, loss: 688.812288\n",
      "Epoch 62, loss: 689.063563\n",
      "Epoch 63, loss: 688.756363\n",
      "Epoch 64, loss: 688.806006\n",
      "Epoch 65, loss: 688.815007\n",
      "Epoch 66, loss: 688.468592\n",
      "Epoch 67, loss: 688.520871\n",
      "Epoch 68, loss: 688.861531\n",
      "Epoch 69, loss: 688.421013\n",
      "Epoch 70, loss: 688.361838\n",
      "Epoch 71, loss: 688.610363\n",
      "Epoch 72, loss: 688.567445\n",
      "Epoch 73, loss: 688.271204\n",
      "Epoch 74, loss: 688.544593\n",
      "Epoch 75, loss: 688.112052\n",
      "Epoch 76, loss: 688.481885\n",
      "Epoch 77, loss: 688.426423\n",
      "Epoch 78, loss: 688.344052\n",
      "Epoch 79, loss: 688.322048\n",
      "Epoch 80, loss: 688.283761\n",
      "Epoch 81, loss: 688.277605\n",
      "Epoch 82, loss: 688.140636\n",
      "Epoch 83, loss: 688.310521\n",
      "Epoch 84, loss: 688.049060\n",
      "Epoch 85, loss: 688.002073\n",
      "Epoch 86, loss: 688.221731\n",
      "Epoch 87, loss: 688.081836\n",
      "Epoch 88, loss: 687.760855\n",
      "Epoch 89, loss: 687.703044\n",
      "Epoch 90, loss: 687.907907\n",
      "Epoch 91, loss: 687.926429\n",
      "Epoch 92, loss: 688.058402\n",
      "Epoch 93, loss: 687.652432\n",
      "Epoch 94, loss: 688.028406\n",
      "Epoch 95, loss: 687.859310\n",
      "Epoch 96, loss: 687.460085\n",
      "Epoch 97, loss: 687.617790\n",
      "Epoch 98, loss: 687.636570\n",
      "Epoch 99, loss: 687.877186\n",
      "Epoch 100, loss: 687.655472\n",
      "Epoch 101, loss: 687.511615\n",
      "Epoch 102, loss: 687.682472\n",
      "Epoch 103, loss: 687.693937\n",
      "Epoch 104, loss: 687.570213\n",
      "Epoch 105, loss: 687.394287\n",
      "Epoch 106, loss: 687.529599\n",
      "Epoch 107, loss: 687.300158\n",
      "Epoch 108, loss: 687.330492\n",
      "Epoch 109, loss: 687.596832\n",
      "Epoch 110, loss: 687.482160\n",
      "Epoch 111, loss: 687.346322\n",
      "Epoch 112, loss: 687.024398\n",
      "Epoch 113, loss: 687.284663\n",
      "Epoch 114, loss: 687.373020\n",
      "Epoch 115, loss: 687.077310\n",
      "Epoch 116, loss: 687.416360\n",
      "Epoch 117, loss: 686.916375\n",
      "Epoch 118, loss: 687.036389\n",
      "Epoch 119, loss: 687.147808\n",
      "Epoch 120, loss: 686.910129\n",
      "Epoch 121, loss: 687.290849\n",
      "Epoch 122, loss: 687.179621\n",
      "Epoch 123, loss: 686.787184\n",
      "Epoch 124, loss: 686.942037\n",
      "Epoch 125, loss: 687.074653\n",
      "Epoch 126, loss: 686.971882\n",
      "Epoch 127, loss: 686.960014\n",
      "Epoch 128, loss: 687.073724\n",
      "Epoch 129, loss: 686.807158\n",
      "Epoch 130, loss: 687.000205\n",
      "Epoch 131, loss: 686.953797\n",
      "Epoch 132, loss: 686.798418\n",
      "Epoch 133, loss: 686.582408\n",
      "Epoch 134, loss: 686.700482\n",
      "Epoch 135, loss: 686.618248\n",
      "Epoch 136, loss: 686.628377\n",
      "Epoch 137, loss: 686.968629\n",
      "Epoch 138, loss: 686.542955\n",
      "Epoch 139, loss: 686.555376\n",
      "Epoch 140, loss: 686.612151\n",
      "Epoch 141, loss: 686.388104\n",
      "Epoch 142, loss: 686.328080\n",
      "Epoch 143, loss: 686.323928\n",
      "Epoch 144, loss: 686.655925\n",
      "Epoch 145, loss: 686.268589\n",
      "Epoch 146, loss: 686.604965\n",
      "Epoch 147, loss: 686.592886\n",
      "Epoch 148, loss: 686.332470\n",
      "Epoch 149, loss: 686.118929\n",
      "Epoch 150, loss: 686.361432\n",
      "Epoch 151, loss: 686.301837\n",
      "Epoch 152, loss: 686.274568\n",
      "Epoch 153, loss: 686.296453\n",
      "Epoch 154, loss: 686.241932\n",
      "Epoch 155, loss: 686.341509\n",
      "Epoch 156, loss: 686.329723\n",
      "Epoch 157, loss: 686.275475\n",
      "Epoch 158, loss: 686.230532\n",
      "Epoch 159, loss: 686.058039\n",
      "Epoch 160, loss: 686.347353\n",
      "Epoch 161, loss: 686.020405\n",
      "Epoch 162, loss: 686.023181\n",
      "Epoch 163, loss: 686.189095\n",
      "Epoch 164, loss: 685.967572\n",
      "Epoch 165, loss: 685.660789\n",
      "Epoch 166, loss: 686.176404\n",
      "Epoch 167, loss: 685.832694\n",
      "Epoch 168, loss: 685.941053\n",
      "Epoch 169, loss: 685.757225\n",
      "Epoch 170, loss: 685.999622\n",
      "Epoch 171, loss: 685.653794\n",
      "Epoch 172, loss: 685.994729\n",
      "Epoch 173, loss: 685.500180\n",
      "Epoch 174, loss: 685.824240\n",
      "Epoch 175, loss: 685.931478\n",
      "Epoch 176, loss: 685.623289\n",
      "Epoch 177, loss: 685.424702\n",
      "Epoch 178, loss: 685.850056\n",
      "Epoch 179, loss: 685.514192\n",
      "Epoch 180, loss: 685.454404\n",
      "Epoch 181, loss: 685.473762\n",
      "Epoch 182, loss: 685.702361\n",
      "Epoch 183, loss: 685.449298\n",
      "Epoch 184, loss: 685.571206\n",
      "Epoch 185, loss: 685.926394\n",
      "Epoch 186, loss: 685.291258\n",
      "Epoch 187, loss: 685.480831\n",
      "Epoch 188, loss: 685.689809\n",
      "Epoch 189, loss: 685.194414\n",
      "Epoch 190, loss: 685.160306\n",
      "Epoch 191, loss: 685.493812\n",
      "Epoch 192, loss: 685.167487\n",
      "Epoch 193, loss: 685.111228\n",
      "Epoch 194, loss: 684.960328\n",
      "Epoch 195, loss: 685.239986\n",
      "Epoch 196, loss: 685.117405\n",
      "Epoch 197, loss: 685.492562\n",
      "Epoch 198, loss: 685.132138\n",
      "Epoch 199, loss: 685.213430\n",
      "Accuracy: 0.213000, learning rate: 0.000100, reg strength: 0.000001\n",
      "Accuracy: 0.207000, learning rate: 0.000100, reg strength: 0.000100\n",
      "Accuracy: 0.206000, learning rate: 0.000100, reg strength: 0.000010\n",
      "Accuracy: 0.206000, learning rate: 0.000010, reg strength: 0.000100\n",
      "Accuracy: 0.206000, learning rate: 0.000010, reg strength: 0.000010\n",
      "Accuracy: 0.206000, learning rate: 0.000010, reg strength: 0.000001\n",
      "Accuracy: 0.206000, learning rate: 0.000010, reg strength: 0.000000\n",
      "Accuracy: 0.206000, learning rate: 0.000001, reg strength: 0.000100\n",
      "Accuracy: 0.206000, learning rate: 0.000001, reg strength: 0.000010\n",
      "Accuracy: 0.206000, learning rate: 0.000001, reg strength: 0.000001\n",
      "Accuracy: 0.204000, learning rate: 0.000100, reg strength: 0.000000\n",
      "Accuracy: 0.202000, learning rate: 0.000001, reg strength: 0.000000\n",
      "Accuracy: 0.169000, learning rate: 0.001000, reg strength: 0.000000\n",
      "Accuracy: 0.146000, learning rate: 0.001000, reg strength: 0.000100\n",
      "Accuracy: 0.118000, learning rate: 0.001000, reg strength: 0.000010\n",
      "Accuracy: 0.086000, learning rate: 0.001000, reg strength: 0.000001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5, 1e-6]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6, 1e-7]\n",
    "\n",
    "results = []\n",
    "for lr in learning_rates:\n",
    "    for rs in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=lr, batch_size=batch_size, reg=rs)\n",
    "\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "\n",
    "        results.append((accuracy, lr, rs))\n",
    "\n",
    "results.sort(key=lambda x: x[0], reverse = True)\n",
    "\n",
    "for result in results:\n",
    "    print(\"Accuracy: %f, learning rate: %f, reg strength: %f\" % result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 691.228552\n",
      "Epoch 1, loss: 690.111231\n",
      "Epoch 2, loss: 689.605051\n",
      "Epoch 3, loss: 689.508944\n",
      "Epoch 4, loss: 688.563788\n",
      "Epoch 5, loss: 688.987323\n",
      "Epoch 6, loss: 688.264936\n",
      "Epoch 7, loss: 687.906390\n",
      "Epoch 8, loss: 687.758693\n",
      "Epoch 9, loss: 687.241499\n",
      "Epoch 10, loss: 687.803391\n",
      "Epoch 11, loss: 687.293523\n",
      "Epoch 12, loss: 686.938214\n",
      "Epoch 13, loss: 686.379923\n",
      "Epoch 14, loss: 686.211990\n",
      "Epoch 15, loss: 685.880932\n",
      "Epoch 16, loss: 685.400938\n",
      "Epoch 17, loss: 685.298200\n",
      "Epoch 18, loss: 685.650973\n",
      "Epoch 19, loss: 685.075083\n",
      "Epoch 20, loss: 684.357766\n",
      "Epoch 21, loss: 684.616243\n",
      "Epoch 22, loss: 684.547610\n",
      "Epoch 23, loss: 683.766468\n",
      "Epoch 24, loss: 684.095723\n",
      "Epoch 25, loss: 683.881756\n",
      "Epoch 26, loss: 683.749429\n",
      "Epoch 27, loss: 683.435497\n",
      "Epoch 28, loss: 683.516467\n",
      "Epoch 29, loss: 682.338695\n",
      "Epoch 30, loss: 682.329847\n",
      "Epoch 31, loss: 682.193330\n",
      "Epoch 32, loss: 682.784157\n",
      "Epoch 33, loss: 682.466266\n",
      "Epoch 34, loss: 681.305935\n",
      "Epoch 35, loss: 682.258331\n",
      "Epoch 36, loss: 681.217934\n",
      "Epoch 37, loss: 682.312288\n",
      "Epoch 38, loss: 681.184080\n",
      "Epoch 39, loss: 681.615073\n",
      "Epoch 40, loss: 681.272933\n",
      "Epoch 41, loss: 681.035310\n",
      "Epoch 42, loss: 680.344971\n",
      "Epoch 43, loss: 680.699212\n",
      "Epoch 44, loss: 679.921542\n",
      "Epoch 45, loss: 680.059367\n",
      "Epoch 46, loss: 679.218548\n",
      "Epoch 47, loss: 679.913991\n",
      "Epoch 48, loss: 679.977302\n",
      "Epoch 49, loss: 679.853017\n",
      "Epoch 50, loss: 680.056639\n",
      "Epoch 51, loss: 679.927993\n",
      "Epoch 52, loss: 678.907700\n",
      "Epoch 53, loss: 679.360382\n",
      "Epoch 54, loss: 679.260630\n",
      "Epoch 55, loss: 678.544362\n",
      "Epoch 56, loss: 678.756751\n",
      "Epoch 57, loss: 679.087976\n",
      "Epoch 58, loss: 678.420776\n",
      "Epoch 59, loss: 678.179203\n",
      "Epoch 60, loss: 678.337313\n",
      "Epoch 61, loss: 678.663298\n",
      "Epoch 62, loss: 678.931355\n",
      "Epoch 63, loss: 677.901144\n",
      "Epoch 64, loss: 677.772849\n",
      "Epoch 65, loss: 678.555939\n",
      "Epoch 66, loss: 678.714623\n",
      "Epoch 67, loss: 677.803341\n",
      "Epoch 68, loss: 677.708468\n",
      "Epoch 69, loss: 677.670969\n",
      "Epoch 70, loss: 677.750754\n",
      "Epoch 71, loss: 677.511056\n",
      "Epoch 72, loss: 676.818431\n",
      "Epoch 73, loss: 677.910344\n",
      "Epoch 74, loss: 677.151434\n",
      "Epoch 75, loss: 676.926612\n",
      "Epoch 76, loss: 677.246527\n",
      "Epoch 77, loss: 676.513374\n",
      "Epoch 78, loss: 676.993210\n",
      "Epoch 79, loss: 676.383191\n",
      "Epoch 80, loss: 676.644567\n",
      "Epoch 81, loss: 676.949171\n",
      "Epoch 82, loss: 676.618938\n",
      "Epoch 83, loss: 675.706193\n",
      "Epoch 84, loss: 676.741478\n",
      "Epoch 85, loss: 676.643346\n",
      "Epoch 86, loss: 676.504281\n",
      "Epoch 87, loss: 676.278332\n",
      "Epoch 88, loss: 675.458150\n",
      "Epoch 89, loss: 676.191554\n",
      "Epoch 90, loss: 676.170763\n",
      "Epoch 91, loss: 676.278505\n",
      "Epoch 92, loss: 675.601047\n",
      "Epoch 93, loss: 675.626200\n",
      "Epoch 94, loss: 675.116569\n",
      "Epoch 95, loss: 676.097595\n",
      "Epoch 96, loss: 676.111589\n",
      "Epoch 97, loss: 675.703830\n",
      "Epoch 98, loss: 675.096551\n",
      "Epoch 99, loss: 675.060918\n",
      "Epoch 100, loss: 675.133126\n",
      "Epoch 101, loss: 674.908957\n",
      "Epoch 102, loss: 675.704550\n",
      "Epoch 103, loss: 674.918730\n",
      "Epoch 104, loss: 674.668808\n",
      "Epoch 105, loss: 675.024132\n",
      "Epoch 106, loss: 674.413856\n",
      "Epoch 107, loss: 675.844415\n",
      "Epoch 108, loss: 675.248281\n",
      "Epoch 109, loss: 675.184036\n",
      "Epoch 110, loss: 674.990176\n",
      "Epoch 111, loss: 675.233979\n",
      "Epoch 112, loss: 675.005708\n",
      "Epoch 113, loss: 674.823336\n",
      "Epoch 114, loss: 674.500775\n",
      "Epoch 115, loss: 674.816886\n",
      "Epoch 116, loss: 674.820079\n",
      "Epoch 117, loss: 674.509408\n",
      "Epoch 118, loss: 674.581481\n",
      "Epoch 119, loss: 674.371496\n",
      "Epoch 120, loss: 675.253791\n",
      "Epoch 121, loss: 674.172743\n",
      "Epoch 122, loss: 674.635043\n",
      "Epoch 123, loss: 674.444401\n",
      "Epoch 124, loss: 673.915749\n",
      "Epoch 125, loss: 675.155975\n",
      "Epoch 126, loss: 674.054605\n",
      "Epoch 127, loss: 674.598635\n",
      "Epoch 128, loss: 674.305023\n",
      "Epoch 129, loss: 674.466174\n",
      "Epoch 130, loss: 674.544283\n",
      "Epoch 131, loss: 674.444763\n",
      "Epoch 132, loss: 674.722612\n",
      "Epoch 133, loss: 673.922114\n",
      "Epoch 134, loss: 673.303091\n",
      "Epoch 135, loss: 673.722600\n",
      "Epoch 136, loss: 674.140399\n",
      "Epoch 137, loss: 674.623145\n",
      "Epoch 138, loss: 674.102827\n",
      "Epoch 139, loss: 674.322384\n",
      "Epoch 140, loss: 674.213677\n",
      "Epoch 141, loss: 674.566575\n",
      "Epoch 142, loss: 673.602165\n",
      "Epoch 143, loss: 674.270183\n",
      "Epoch 144, loss: 674.766853\n",
      "Epoch 145, loss: 674.031673\n",
      "Epoch 146, loss: 673.901531\n",
      "Epoch 147, loss: 674.259915\n",
      "Epoch 148, loss: 673.903190\n",
      "Epoch 149, loss: 673.596054\n",
      "Epoch 150, loss: 673.549452\n",
      "Epoch 151, loss: 674.057596\n",
      "Epoch 152, loss: 674.442440\n",
      "Epoch 153, loss: 673.441578\n",
      "Epoch 154, loss: 672.976963\n",
      "Epoch 155, loss: 673.709004\n",
      "Epoch 156, loss: 674.131661\n",
      "Epoch 157, loss: 673.026501\n",
      "Epoch 158, loss: 673.868901\n",
      "Epoch 159, loss: 673.546354\n",
      "Epoch 160, loss: 673.782799\n",
      "Epoch 161, loss: 673.869093\n",
      "Epoch 162, loss: 674.076945\n",
      "Epoch 163, loss: 674.183648\n",
      "Epoch 164, loss: 673.518321\n",
      "Epoch 165, loss: 672.554244\n",
      "Epoch 166, loss: 673.556621\n",
      "Epoch 167, loss: 674.358689\n",
      "Epoch 168, loss: 674.114355\n",
      "Epoch 169, loss: 673.557808\n",
      "Epoch 170, loss: 673.832033\n",
      "Epoch 171, loss: 673.592197\n",
      "Epoch 172, loss: 673.610938\n",
      "Epoch 173, loss: 672.894841\n",
      "Epoch 174, loss: 673.820201\n",
      "Epoch 175, loss: 673.523196\n",
      "Epoch 176, loss: 674.294986\n",
      "Epoch 177, loss: 673.790600\n",
      "Epoch 178, loss: 673.654963\n",
      "Epoch 179, loss: 672.718301\n",
      "Epoch 180, loss: 673.250237\n",
      "Epoch 181, loss: 674.295359\n",
      "Epoch 182, loss: 673.606153\n",
      "Epoch 183, loss: 672.714662\n",
      "Epoch 184, loss: 673.870404\n",
      "Epoch 185, loss: 673.095748\n",
      "Epoch 186, loss: 673.740474\n",
      "Epoch 187, loss: 673.505175\n",
      "Epoch 188, loss: 673.647920\n",
      "Epoch 189, loss: 673.224010\n",
      "Epoch 190, loss: 673.308471\n",
      "Epoch 191, loss: 673.276459\n",
      "Epoch 192, loss: 672.813207\n",
      "Epoch 193, loss: 674.197667\n",
      "Epoch 194, loss: 673.728821\n",
      "Epoch 195, loss: 673.292059\n",
      "Epoch 196, loss: 673.979082\n",
      "Epoch 197, loss: 673.978245\n",
      "Epoch 198, loss: 673.035311\n",
      "Epoch 199, loss: 673.389756\n",
      "Linear softmax classifier test set accuracy: 0.182000\n"
     ]
    }
   ],
   "source": [
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "classifier.fit(train_X, train_y, epochs=200, learning_rate=1e-5, batch_size=300, reg=1e-6)\n",
    "\n",
    "test_pred = classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
